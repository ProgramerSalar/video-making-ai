{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed485de78406453e8111ecd5872d933c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manjusha Kumari\\.cache\\huggingface\\hub\\models--Lightricks--LTX-Video. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module diffusers has no attribute LTXPipeline",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionPipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightricks/LTX-Video\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAstronaut in a jungle, cold color palette, muted colors, detailed, 8k\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m pipe(prompt)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:725\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    722\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe provided pretrained_model_name_or_path \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    723\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    724\u001b[0m         )\n\u001b[1;32m--> 725\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1375\u001b[0m, in \u001b[0;36mDiffusionPipeline.download\u001b[1;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contains custom code in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.py, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(k,\u001b[38;5;250m \u001b[39mv)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,v\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcustom_components\u001b[38;5;241m.\u001b[39mitems()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which must be executed to correctly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload the model. You can inspect the repository content at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,v\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcustom_components\u001b[38;5;241m.\u001b[39mitems()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m     )\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# retrieve passed components that should not be downloaded\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m pipeline_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_pipeline_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_pipe_from_hub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhub_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_class_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1386\u001b[0m expected_components, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_signature_keys(pipeline_class)\n\u001b[0;32m   1387\u001b[0m passed_components \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m expected_components \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs]\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:370\u001b[0m, in \u001b[0;36m_get_pipeline_class\u001b[1;34m(class_obj, config, load_connected_pipeline, custom_pipeline, repo_id, hub_revision, class_name, cache_dir, revision)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class name could not be found in the configuration file. Please make sure to pass the correct `class_name`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[0;32m    368\u001b[0m class_name \u001b[38;5;241m=\u001b[39m class_name[\u001b[38;5;241m4\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m class_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlax\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m class_name\n\u001b[1;32m--> 370\u001b[0m pipeline_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdiffusers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_connected_pipeline:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_connected_pipeline\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\utils\\import_utils.py:846\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    844\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mAttributeError\u001b[0m: module diffusers has no attribute LTXPipeline"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"Lightricks/LTX-Video\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipe(prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_linear_nd(x):\n",
    "    \"\"\"\n",
    "    Reshape the tensor to (batch_size, -1) suitable for a fully connected layer.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (batch_size, *).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped tensor of shape (batch_size, -1).\n",
    "    \"\"\"\n",
    "    return x.view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DualConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n",
    "        super(DualConv3d, self).__init__()\n",
    "        kernel_size = kernel_size\n",
    "        \n",
    "        if kernel_size == (1, 1, 1):\n",
    "            raise ValueError(\"Kernel size should be greater than 1. Use make_linear_nd instead.\")\n",
    "        \n",
    "        weight1 = nn.Parameter(torch.randn(out_channels, in_channels, *kernel_size))\n",
    "        bias1 = nn.Parameter(torch.randn(out_channels)) if bias else None\n",
    "        stride1 = stride\n",
    "        padding1 = padding\n",
    "        dilation1 = (1, 1, 1)\n",
    "        groups = 1\n",
    "\n",
    "    def forward(self, x, use_conv3d=False):\n",
    "        if kernel_size == (1, 1, 1):\n",
    "            x = make_linear_nd(x)\n",
    "            # Here, you would normally pass this reshaped tensor to a linear layer\n",
    "            return x\n",
    "        else:\n",
    "            x = F.conv3d(x, weight1, bias1, stride1, padding1, dilation1, groups)\n",
    "            return x\n",
    "\n",
    "# Example usage\n",
    "in_channels = 3\n",
    "out_channels = 5\n",
    "kernel_size = (3, 3, 3)\n",
    "stride = (2, 2, 2)\n",
    "padding = (1, 1, 1)\n",
    "\n",
    "# Create an instance of DualConv3d\n",
    "dual_conv3d = DualConv3d(in_channels, out_channels, kernel_size, stride, padding, bias=True)\n",
    "\n",
    "# Example input tensor\n",
    "test_input = torch.randn(1, 3, 10, 10, 10)\n",
    "\n",
    "# Perform forward pass\n",
    "output = dual_conv3d(test_input, use_conv3d=True)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import torch \n",
    "\n",
    "# a = torch.sqrt(2)\n",
    "# print(a)\n",
    "\n",
    "b = math.sqrt(2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor a:\n",
      "tensor([[[[[2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.]]]]])\n",
      "Original Tensor b:\n",
      "tensor([[[[[5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.]]]]])\n",
      "blend_extent 2\n",
      "y 0\n",
      "object: ->  tensor([[[[5., 5., 5., 5.]]]])\n",
      "calulate -2\n",
      "blend extend 2\n",
      "blend_extent2:  tensor([[[[[2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.]]]]])\n",
      "blend_extent3:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent4:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent5:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent6:  tensor([[[[7., 7., 7., 7.]]]])\n",
      "multiple tensor([[[[0., 0., 0., 0.]]]])\n",
      "blend_extent7:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "hello: tensor([[[[5., 5., 5., 5.]]]]) = tensor([[[[2., 2., 2., 2.]]]])\n",
      "y 1\n",
      "object: ->  tensor([[[[5., 5., 5., 5.]]]])\n",
      "calulate -1\n",
      "blend extend 2\n",
      "blend_extent2:  tensor([[[[[2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.]]]]])\n",
      "blend_extent3:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent4:  tensor([[[[0., 0., 0., 0.]]]])\n",
      "blend_extent5:  tensor([[[[1., 1., 1., 1.]]]])\n",
      "blend_extent6:  tensor([[[[6., 6., 6., 6.]]]])\n",
      "multiple tensor([[[[5., 5., 5., 5.]]]])\n",
      "blend_extent7:  tensor([[[[6., 6., 6., 6.]]]])\n",
      "hello: tensor([[[[5., 5., 5., 5.]]]]) = tensor([[[[2., 2., 2., 2.]]]])\n",
      "Blended Tensor:\n",
      "tensor([[[[[5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.]]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def blend_v(\n",
    "        a: torch.Tensor,\n",
    "        b: torch.Tensor,\n",
    "        blend_extent: int\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    blend_extent = min(a.shape[3], b.shape[3], blend_extent)\n",
    "    print(\"blend_extent\", blend_extent)\n",
    "\n",
    "    for y in range(blend_extent):\n",
    "        print(\"y\", y)\n",
    "        print(\"object: -> \", b[:, :, :, y, :])\n",
    "        print(\"calulate\", -blend_extent + y)\n",
    "        # print(\"blend extent2 \", a.size()) -> torch.Size([1, 1, 1, 4, 4])\n",
    "        print(\"blend extend\", blend_extent)\n",
    "        print(\"blend_extent2: \", a[:, :, :, :, :])\n",
    "        print(\"blend_extent3: \", a[:, :, :, -blend_extent + y, :])   # tensor([[[[2., 2., 2., 2.]]]])\n",
    "        print(\"blend_extent4: \", a[:, :, :, -blend_extent + y, :] * (1 - y))  # y = 0 -> 1 - 0 = 1 But y = 1 -> 1 - 1 = 0\n",
    "        print(\"blend_extent5: \", a[:, :, :, -blend_extent + y] * (1 - y / blend_extent))  # 1 / 2 = 1 and 0 / 2 = 0 \n",
    "        # tensor([[[[2., 2., 2., 2.]]]]) + tensor([[[[5., 5., 5., 5.]]]])  =  tensor([[[[7., 7., 7., 7.]]]])\n",
    "        print(\"blend_extent6: \", a[:, :, :, -blend_extent + y] * (1 - y / blend_extent) + b[:, :, :, y, :]) \n",
    "        print(\"multiple\", b[:, :, :, y, :] * y) # 5 * 0 = 0 and 5 * 1 = 5 \n",
    "        print(\"blend_extent7: \", a[:, :, :, -blend_extent + y] * (1 - y / blend_extent) + b[:, :, :, y, :] * y)  # 5 * 0 = 0 and 5 * 1 = 5 \n",
    "        \n",
    "\n",
    "        \n",
    "        print(f\"hello: {b[:, :, :, y, :]} = {a[:, :, :, -blend_extent + y, :]}\")\n",
    "\n",
    "    return b\n",
    "\n",
    "# Create sample tensors\n",
    "a = torch.ones((1, 1, 1, 4, 4)) * 2  # Tensor a filled with value 2\n",
    "b = torch.ones((1, 1, 1, 4, 4)) * 5  # Tensor b filled with value 5\n",
    "\n",
    "# Display original tensors\n",
    "print(\"Original Tensor a:\")\n",
    "print(a)\n",
    "\n",
    "print(\"Original Tensor b:\")\n",
    "print(b)\n",
    "\n",
    "# Set blend_extent\n",
    "blend_extent = 2\n",
    "\n",
    "# Blend tensors vertically\n",
    "blended_tensor = blend_v(a, b, blend_extent)\n",
    "\n",
    "# Display blended tensor\n",
    "print(\"Blended Tensor:\")\n",
    "print(blended_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_v(\n",
    "            self,\n",
    "            a: torch.Tensor,\n",
    "            b: torch.Tensor,\n",
    "            blend_extent: int\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        blend_extent = min(a.shape[3],\n",
    "                           b.shape[3],\n",
    "                           blend_extent\n",
    "                           )\n",
    "        \n",
    "\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (\n",
    "                1 - y / blend_extent\n",
    "            ) + b[:, :, :, y, :] * (y / blend_extent)\n",
    "\n",
    "\n",
    "        return b \n",
    "    \n",
    "\n",
    "def blend_h(\n",
    "            self,\n",
    "            a: torch.Tensor,\n",
    "            b: torch.Tensor,\n",
    "            blend_extent:  int\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        blend_extent = min(a.shape[4], b.shape[4], blend_extent)\n",
    "\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (\n",
    "                1 - x / blend_extent\n",
    "            ) + b[:, :, :, :, x] * (x / blend_extent)\n",
    "    \n",
    "\n",
    "        return b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "overlap_size:  3\n",
      "blend_extent:  1\n",
      "row_limit:  3\n",
      "i:  0\n",
      "j:  0\n",
      "before tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "before add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "          [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "          [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "          [-0.3214, -0.6163,  0.8686, -1.5710]],\n",
      "\n",
      "         [[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "          [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "          [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "          [-1.4594, -1.3954, -0.1143,  0.5692]],\n",
      "\n",
      "         [[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "          [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "          [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "          [-1.5739, -0.4865,  1.5061,  0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[-0.1515, -0.4670,  0.2821,  0.7793],\n",
      "          [ 0.5981, -0.0590,  0.2559, -0.4094],\n",
      "          [-0.0795, -0.4326, -0.2921,  0.1694],\n",
      "          [-0.2159,  0.0672, -0.1232, -0.8193]],\n",
      "\n",
      "         [[ 0.0028,  0.3754,  0.0967, -0.2321],\n",
      "          [ 0.0911, -0.0075, -0.2960,  0.5578],\n",
      "          [-0.2440, -0.1506,  0.1978,  0.4953],\n",
      "          [-0.5388,  0.2812,  0.3634,  0.9035]],\n",
      "\n",
      "         [[ 0.4003,  0.2176, -0.2258, -0.0688],\n",
      "          [-0.1504,  0.2110, -0.3487,  0.6612],\n",
      "          [-1.2308, -0.1979,  0.7831, -1.0373],\n",
      "          [-0.2329, -0.7870, -0.0583, -0.3236]],\n",
      "\n",
      "         [[-0.5736, -1.1146, -0.0926,  0.3791],\n",
      "          [-1.3218, -0.1257, -0.2714, -0.1148],\n",
      "          [-0.3103, -0.1509, -0.0782,  0.7312],\n",
      "          [ 0.3630, -0.2393,  0.4769,  0.7472]],\n",
      "\n",
      "         [[-0.0914,  0.5633,  0.2613,  0.2330],\n",
      "          [ 0.1870,  0.5642, -0.7003, -0.6296],\n",
      "          [-0.1041, -0.6836, -0.3631,  0.4847],\n",
      "          [-0.0754, -0.1411,  0.1807,  0.5265]],\n",
      "\n",
      "         [[-0.5094, -0.5004, -0.0652,  0.0996],\n",
      "          [ 0.2430,  0.1559,  0.3274, -0.0515],\n",
      "          [ 0.6533, -0.0671,  0.5773,  0.7474],\n",
      "          [ 0.9911,  0.5063,  0.3010, -0.0045]],\n",
      "\n",
      "         [[ 0.8989,  0.0880,  0.2075,  0.6340],\n",
      "          [ 0.3365,  0.3409,  0.3967,  0.3274],\n",
      "          [-0.0387,  1.2388,  0.4723, -0.9630],\n",
      "          [ 0.6008,  0.2854,  0.2148,  0.0209]],\n",
      "\n",
      "         [[ 0.5665,  0.2106, -0.1043, -0.1559],\n",
      "          [ 0.3953, -0.1291,  0.7229,  0.4070],\n",
      "          [ 0.1641, -0.5198,  0.8538, -0.9724],\n",
      "          [-0.0484, -0.4151,  0.1318,  0.4480]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)]\n",
      "j:  3\n",
      "before tile:  tensor([[[[[ 0.1764],\n",
      "           [-1.2856],\n",
      "           [-1.5204],\n",
      "           [-1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-0.0374],\n",
      "           [ 0.9624],\n",
      "           [-1.4286],\n",
      "           [ 0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0122],\n",
      "           [ 1.2837],\n",
      "           [-0.8858],\n",
      "           [ 0.1685]]]]])\n",
      "before add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[ 0.1764],\n",
      "          [-1.2856],\n",
      "          [-1.5204],\n",
      "          [-1.5710]],\n",
      "\n",
      "         [[-0.0374],\n",
      "          [ 0.9624],\n",
      "          [-1.4286],\n",
      "          [ 0.5692]],\n",
      "\n",
      "         [[ 0.0122],\n",
      "          [ 1.2837],\n",
      "          [-0.8858],\n",
      "          [ 0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[ 0.4931],\n",
      "          [-0.5834],\n",
      "          [ 0.3503],\n",
      "          [-0.5842]],\n",
      "\n",
      "         [[ 0.0355],\n",
      "          [ 0.5459],\n",
      "          [ 0.5298],\n",
      "          [ 0.5515]],\n",
      "\n",
      "         [[ 0.1415],\n",
      "          [-0.1029],\n",
      "          [-0.3908],\n",
      "          [-0.1082]],\n",
      "\n",
      "         [[ 0.2223],\n",
      "          [-0.1802],\n",
      "          [ 0.2950],\n",
      "          [ 0.8458]],\n",
      "\n",
      "         [[ 0.0744],\n",
      "          [ 0.0293],\n",
      "          [ 0.1612],\n",
      "          [-0.0822]],\n",
      "\n",
      "         [[ 0.1301],\n",
      "          [-0.0904],\n",
      "          [ 0.3509],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[ 0.3890],\n",
      "          [-0.0360],\n",
      "          [-0.0852],\n",
      "          [ 0.0974]],\n",
      "\n",
      "         [[-0.1284],\n",
      "          [ 0.1792],\n",
      "          [-0.3948],\n",
      "          [ 0.0181]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "rows:  [[tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)]]\n",
      "i:  3\n",
      "j:  0\n",
      "before tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "before add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[-0.3214, -0.6163,  0.8686, -1.5710]],\n",
      "\n",
      "         [[-1.4594, -1.3954, -0.1143,  0.5692]],\n",
      "\n",
      "         [[-1.5739, -0.4865,  1.5061,  0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[ 0.1617, -0.0016,  0.1255, -0.3699]],\n",
      "\n",
      "         [[-0.2799, -0.2459, -0.4161,  0.5723]],\n",
      "\n",
      "         [[-0.2936, -0.6766, -0.6531, -0.1541]],\n",
      "\n",
      "         [[ 0.3507,  0.4176,  0.2967, -0.0978]],\n",
      "\n",
      "         [[-0.3842,  0.0382,  0.1290, -0.1349]],\n",
      "\n",
      "         [[ 0.5060,  0.7361,  0.1276, -0.0778]],\n",
      "\n",
      "         [[ 0.1075, -0.2011,  0.2138,  0.1081]],\n",
      "\n",
      "         [[-0.1419, -0.6838, -0.2915,  0.4977]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)]\n",
      "j:  3\n",
      "before tile:  tensor([[[[[-1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1685]]]]])\n",
      "before add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[-1.5710]],\n",
      "\n",
      "         [[ 0.5692]],\n",
      "\n",
      "         [[ 0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[-0.3839]],\n",
      "\n",
      "         [[ 0.4825]],\n",
      "\n",
      "         [[-0.2518]],\n",
      "\n",
      "         [[ 0.2341]],\n",
      "\n",
      "         [[-0.2903]],\n",
      "\n",
      "         [[ 0.1898]],\n",
      "\n",
      "         [[-0.1427]],\n",
      "\n",
      "         [[ 0.1970]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "rows:  [[tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)], [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)]]\n",
      "i:  0\n",
      "row:  [tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "j:  0\n",
      "tile:  tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[ 0.2532,  0.7386,  0.2219],\n",
      "          [ 0.3998,  0.2536, -0.1874],\n",
      "          [ 0.1930,  0.2137, -0.2245]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834],\n",
      "          [-0.7181, -0.1948, -0.6362],\n",
      "          [-0.9344, -0.2942, -0.2364]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724],\n",
      "          [ 0.0147,  0.3724,  0.2178],\n",
      "          [ 0.6500,  0.3631,  0.1851]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471],\n",
      "          [-0.2169, -0.0104, -0.0123],\n",
      "          [-0.1756,  0.2040,  0.0609]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841],\n",
      "          [ 0.5858,  0.1225,  0.1780],\n",
      "          [ 0.3924, -0.3587, -0.1027]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647],\n",
      "          [-0.2310,  0.0728,  0.1766],\n",
      "          [-0.3059,  0.1741,  0.5677]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191],\n",
      "          [-0.5909, -0.2410, -0.4142],\n",
      "          [-0.4341,  0.3380, -0.5479]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749],\n",
      "          [ 0.0786, -0.1515,  0.2320],\n",
      "          [-0.2956,  0.0692,  0.4095]]]], grad_fn=<SliceBackward0>)]\n",
      "j:  1\n",
      "tile:  tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)\n",
      "j is greater then 0:  tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[ 0.2532,  0.7386,  0.2219],\n",
      "          [ 0.3998,  0.2536, -0.1874],\n",
      "          [ 0.1930,  0.2137, -0.2245]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834],\n",
      "          [-0.7181, -0.1948, -0.6362],\n",
      "          [-0.9344, -0.2942, -0.2364]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724],\n",
      "          [ 0.0147,  0.3724,  0.2178],\n",
      "          [ 0.6500,  0.3631,  0.1851]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471],\n",
      "          [-0.2169, -0.0104, -0.0123],\n",
      "          [-0.1756,  0.2040,  0.0609]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841],\n",
      "          [ 0.5858,  0.1225,  0.1780],\n",
      "          [ 0.3924, -0.3587, -0.1027]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647],\n",
      "          [-0.2310,  0.0728,  0.1766],\n",
      "          [-0.3059,  0.1741,  0.5677]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191],\n",
      "          [-0.5909, -0.2410, -0.4142],\n",
      "          [-0.4341,  0.3380, -0.5479]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749],\n",
      "          [ 0.0786, -0.1515,  0.2320],\n",
      "          [-0.2956,  0.0692,  0.4095]]]], grad_fn=<SliceBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565]]]], grad_fn=<SliceBackward0>)]\n",
      "i:  1\n",
      "row:  [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "j:  0\n",
      "tile:  tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "i is greater then 0:  tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[-0.2058,  0.0037,  0.0731]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290]]]], grad_fn=<SliceBackward0>)]\n",
      "j:  1\n",
      "tile:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "i is greater then 0:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "j is greater then 0:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[-0.2058,  0.0037,  0.0731]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290]]]], grad_fn=<SliceBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<SliceBackward0>)]\n",
      "moment:  tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0481],\n",
      "          [ 0.3998,  0.2536, -0.1874,  0.3454],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1252],\n",
      "          [-0.2058,  0.0037,  0.0731,  0.0588]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.1040],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.3530],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.2615],\n",
      "          [-0.4181, -0.5201, -0.3943, -0.3646]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.0861],\n",
      "          [ 0.0147,  0.3724,  0.2178,  0.1800],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.0813],\n",
      "          [ 0.2741,  0.4804,  0.5165,  0.1615]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.0912],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.3859],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1322],\n",
      "          [ 0.2128,  0.2020,  0.0425, -0.2567]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.1034],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.0680],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.1754],\n",
      "          [-0.0567,  0.0205,  0.1008, -0.2048]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.2868],\n",
      "          [-0.2310,  0.0728,  0.1766, -0.1997],\n",
      "          [-0.3059,  0.1741,  0.5677,  0.0036],\n",
      "          [ 0.1920, -0.0642, -0.0410, -0.0165]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.1057],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1596],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.0874],\n",
      "          [-0.2007, -0.1787, -0.0564, -0.1698]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.0838],\n",
      "          [ 0.0786, -0.1515,  0.2320, -0.0340],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.3565],\n",
      "          [-0.1439, -0.5123, -0.3290, -0.0591]]]], grad_fn=<CatBackward0>)\n",
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, tile_sample_min_size, tile_overlap_factor, tile_latent_min_size):\n",
    "        self.tile_sample_min_size = tile_sample_min_size\n",
    "        self.tile_overlap_factor = tile_overlap_factor\n",
    "        self.tile_latent_min_size = tile_latent_min_size\n",
    "        self.encoder = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)  # Dummy encoder\n",
    "        self.quant_conv = nn.Conv2d(8, 8, kernel_size=1, stride=1, padding=0)  # Dummy quant conv\n",
    "\n",
    "    def blend_h(self, left_tile, current_tile, blend_extent):\n",
    "        # Dummy blend horizontal function\n",
    "        return current_tile\n",
    "\n",
    "    def blend_v(self, above_tile, current_tile, blend_extent):\n",
    "        # Dummy blend vertical function\n",
    "        return current_tile\n",
    "\n",
    "    def _hw_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True):\n",
    "        print(\"x\", x)\n",
    "        overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor))  \n",
    "        print(\"overlap_size: \", overlap_size) # 3\n",
    "\n",
    "        blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)\n",
    "        print(\"blend_extent: \", blend_extent)  # 1 \n",
    "\n",
    "\n",
    "        row_limit = self.tile_latent_min_size - blend_extent\n",
    "        print(\"row_limit: \", row_limit)  # torch.Size([1, 8, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[3], overlap_size):\n",
    "            print(\"i: \", i)\n",
    "\n",
    "            row = []\n",
    "            for j in range(0, x.shape[4], overlap_size):\n",
    "                print(\"j: \", j)\n",
    "                \n",
    "                tile = x[:, :, :, i: i + self.tile_sample_min_size, j: j + self.tile_sample_min_size]\n",
    "                print(\"before tile: \", tile)\n",
    "                print(\"before add tile: \", x[:, :, :, i:] )\n",
    "                print(\"after add tile: \", x[:, :, :, i: + self.tile_sample_min_size, ])\n",
    "\n",
    "                # Remove the extra dimension\n",
    "                tile = tile.squeeze(2)\n",
    "                print(\"After tile squeeze: \", tile)\n",
    "\n",
    "                tile = self.encoder(tile)\n",
    "                print(\"tile pass in the encoder: \", tile)\n",
    "\n",
    "                tile = self.quant_conv(tile)\n",
    "                print(\"tile pass in the quant_conv: \", tile)\n",
    "\n",
    "                row.append(tile)\n",
    "                print(\"row: \", row)\n",
    "\n",
    "            rows.append(row)\n",
    "            print(\"rows: \", rows)\n",
    "\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            print(\"i: \", i)\n",
    "            print(\"row: \", row)\n",
    "\n",
    "\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                print(\"j: \", j)\n",
    "                print(\"tile: \", tile)\n",
    "\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                    print(\"i is greater then 0: \", tile)\n",
    "\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                    print(\"j is greater then 0: \", tile)\n",
    "\n",
    "\n",
    "                result_row.append(tile[:, :, :row_limit, :row_limit])\n",
    "                print(\"result_row: \", result_row)\n",
    "\n",
    "            result_rows.append(torch.cat(result_row, dim=3))\n",
    "\n",
    "        moment = torch.cat(result_rows, dim=2)\n",
    "        print(\"moment: \", moment)\n",
    "        return moment\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tile_sample_min_size = 4\n",
    "tile_overlap_factor = 0.25\n",
    "tile_latent_min_size = 4\n",
    "\n",
    "encoder_decoder = EncoderDecoder(tile_sample_min_size, tile_overlap_factor, tile_latent_min_size)\n",
    "\n",
    "# Creating a dummy tensor with shape [1, 3, 1, 4, 4] (batch size, channels, depth, height, width)\n",
    "dummy_tensor = torch.randn(1, 3, 1, 4, 4)\n",
    "\n",
    "# Encode the dummy tensor\n",
    "encoded_tensor = encoder_decoder._hw_tiled_encode(dummy_tensor)\n",
    "\n",
    "print(encoded_tensor.shape)  # Check the shape of the encoded tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@maybe_allow_in_graph\n",
    "class BasicTransformerBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    A basic Transformer block.\n",
    "\n",
    "    Parameters:\n",
    "        dim (`int`): The number of channels in the input and output.\n",
    "        num_attention_heads (`int`): The number of heads to use for multi-head attention.\n",
    "        attention_head_dim (`int`): The number of channels in each head.\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
    "        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.\n",
    "        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n",
    "        num_embeds_ada_norm (:\n",
    "            obj: `int`, *optional*): The number of diffusion steps used during training. See `Transformer2DModel`.\n",
    "        attention_bias (:\n",
    "            obj: `bool`, *optional*, defaults to `False`): Configure if the attentions should contain a bias parameter.\n",
    "        only_cross_attention (`bool`, *optional*):\n",
    "            Whether to use only cross-attention layers. In this case two cross attention layers are used.\n",
    "        double_self_attention (`bool`, *optional*):\n",
    "            Whether to use two self-attention layers. In this case no cross attention layers are used.\n",
    "        upcast_attention (`bool`, *optional*):\n",
    "            Whether to upcast the attention computation to float32. This is useful for mixed precision training.\n",
    "        norm_elementwise_affine (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to use learnable elementwise affine parameters for normalization.\n",
    "        qk_norm (`str`, *optional*, defaults to None):\n",
    "            Set to 'layer_norm' or `rms_norm` to perform query and key normalization.\n",
    "        adaptive_norm (`str`, *optional*, defaults to `\"single_scale_shift\"`):\n",
    "            The type of adaptive norm to use. Can be `\"single_scale_shift\"`, `\"single_scale\"` or \"none\".\n",
    "        standardization_norm (`str`, *optional*, defaults to `\"layer_norm\"`):\n",
    "            The type of pre-normalization to use. Can be `\"layer_norm\"` or `\"rms_norm\"`.\n",
    "        final_dropout (`bool` *optional*, defaults to False):\n",
    "            Whether to apply a final dropout after the last feed-forward layer.\n",
    "        attention_type (`str`, *optional*, defaults to `\"default\"`):\n",
    "            The type of attention to use. Can be `\"default\"` or `\"gated\"` or `\"gated-text-image\"`.\n",
    "        positional_embeddings (`str`, *optional*, defaults to `None`):\n",
    "            The type of positional embeddings to apply to.\n",
    "        num_positional_embeddings (`int`, *optional*, defaults to `None`):\n",
    "            The maximum number of positional embeddings to apply.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_attention_heads: int,\n",
    "        attention_head_dim: int,\n",
    "        dropout=0.0,\n",
    "        cross_attention_dim: Optional[int] = None,\n",
    "        activation_fn: str = \"geglu\",\n",
    "        num_embeds_ada_norm: Optional[int] = None,  # pylint: disable=unused-argument\n",
    "        attention_bias: bool = False,\n",
    "        only_cross_attention: bool = False,\n",
    "        double_self_attention: bool = False,\n",
    "        upcast_attention: bool = False,\n",
    "        norm_elementwise_affine: bool = True,\n",
    "        adaptive_norm: str = \"single_scale_shift\",  # 'single_scale_shift', 'single_scale' or 'none'\n",
    "        standardization_norm: str = \"layer_norm\",  # 'layer_norm' or 'rms_norm'\n",
    "        norm_eps: float = 1e-5,\n",
    "        qk_norm: Optional[str] = None,\n",
    "        final_dropout: bool = False,\n",
    "        attention_type: str = \"default\",  # pylint: disable=unused-argument\n",
    "        ff_inner_dim: Optional[int] = None,\n",
    "        ff_bias: bool = True,\n",
    "        attention_out_bias: bool = True,\n",
    "        use_tpu_flash_attention: bool = False,\n",
    "        use_rope: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.only_cross_attention = only_cross_attention\n",
    "        self.use_tpu_flash_attention = use_tpu_flash_attention\n",
    "        self.adaptive_norm = adaptive_norm\n",
    "\n",
    "        assert standardization_norm in [\"layer_norm\", \"rms_norm\"]\n",
    "        assert adaptive_norm in [\"single_scale_shift\", \"single_scale\", \"none\"]\n",
    "\n",
    "        make_norm_layer = (\n",
    "            nn.LayerNorm if standardization_norm == \"layer_norm\" else RMSNorm\n",
    "        )\n",
    "\n",
    "        # Define 3 blocks. Each block has its own normalization layer.\n",
    "        # 1. Self-Attn\n",
    "        self.norm1 = make_norm_layer(\n",
    "            dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps\n",
    "        )\n",
    "\n",
    "        self.attn1 = Attention(\n",
    "            query_dim=dim,\n",
    "            heads=num_attention_heads,\n",
    "            dim_head=attention_head_dim,\n",
    "            dropout=dropout,\n",
    "            bias=attention_bias,\n",
    "            cross_attention_dim=cross_attention_dim if only_cross_attention else None,\n",
    "            upcast_attention=upcast_attention,\n",
    "            out_bias=attention_out_bias,\n",
    "            use_tpu_flash_attention=use_tpu_flash_attention,\n",
    "            qk_norm=qk_norm,\n",
    "            use_rope=use_rope,\n",
    "        )\n",
    "\n",
    "        # 2. Cross-Attn\n",
    "        if cross_attention_dim is not None or double_self_attention:\n",
    "            self.attn2 = Attention(\n",
    "                query_dim=dim,\n",
    "                cross_attention_dim=(\n",
    "                    cross_attention_dim if not double_self_attention else None\n",
    "                ),\n",
    "                heads=num_attention_heads,\n",
    "                dim_head=attention_head_dim,\n",
    "                dropout=dropout,\n",
    "                bias=attention_bias,\n",
    "                upcast_attention=upcast_attention,\n",
    "                out_bias=attention_out_bias,\n",
    "                use_tpu_flash_attention=use_tpu_flash_attention,\n",
    "                qk_norm=qk_norm,\n",
    "                use_rope=use_rope,\n",
    "            )  # is self-attn if encoder_hidden_states is none\n",
    "\n",
    "            if adaptive_norm == \"none\":\n",
    "                self.attn2_norm = make_norm_layer(\n",
    "                    dim, norm_eps, norm_elementwise_affine\n",
    "                )\n",
    "        else:\n",
    "            self.attn2 = None\n",
    "            self.attn2_norm = None\n",
    "\n",
    "        self.norm2 = make_norm_layer(dim, norm_eps, norm_elementwise_affine)\n",
    "\n",
    "        # 3. Feed-forward\n",
    "        self.ff = FeedForward(\n",
    "            dim,\n",
    "            dropout=dropout,\n",
    "            activation_fn=activation_fn,\n",
    "            final_dropout=final_dropout,\n",
    "            inner_dim=ff_inner_dim,\n",
    "            bias=ff_bias,\n",
    "        )\n",
    "\n",
    "        # 5. Scale-shift for PixArt-Alpha.\n",
    "        if adaptive_norm != \"none\":\n",
    "            num_ada_params = 4 if adaptive_norm == \"single_scale\" else 6\n",
    "            self.scale_shift_table = nn.Parameter(\n",
    "                torch.randn(num_ada_params, dim) / dim**0.5\n",
    "            )\n",
    "\n",
    "        # let chunk size default to None\n",
    "        self._chunk_size = None\n",
    "        self._chunk_dim = 0\n",
    "\n",
    "    def set_use_tpu_flash_attention(self):\n",
    "        r\"\"\"\n",
    "        Function sets the flag in this object and propagates down the children. The flag will enforce the usage of TPU\n",
    "        attention kernel.\n",
    "        \"\"\"\n",
    "        self.use_tpu_flash_attention = True\n",
    "        self.attn1.set_use_tpu_flash_attention()\n",
    "        self.attn2.set_use_tpu_flash_attention()\n",
    "\n",
    "    def set_chunk_feed_forward(self, chunk_size: Optional[int], dim: int = 0):\n",
    "        # Sets chunk feed-forward\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_dim = dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        freqs_cis: Optional[Tuple[torch.FloatTensor, torch.FloatTensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        cross_attention_kwargs: Dict[str, Any] = None,\n",
    "        class_labels: Optional[torch.LongTensor] = None,\n",
    "        added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        if cross_attention_kwargs is not None:\n",
    "            if cross_attention_kwargs.get(\"scale\", None) is not None:\n",
    "                logger.warning(\n",
    "                    \"Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.\"\n",
    "                )\n",
    "\n",
    "        # Notice that normalization is always applied before the real computation in the following blocks.\n",
    "        # 0. Self-Attention\n",
    "        batch_size = hidden_states.shape[0]\n",
    "\n",
    "        norm_hidden_states = self.norm1(hidden_states)\n",
    "\n",
    "        # Apply ada_norm_single\n",
    "        if self.adaptive_norm in [\"single_scale_shift\", \"single_scale\"]:\n",
    "            assert timestep.ndim == 3  # [batch, 1 or num_tokens, embedding_dim]\n",
    "            num_ada_params = self.scale_shift_table.shape[0]\n",
    "            ada_values = self.scale_shift_table[None, None] + timestep.reshape(\n",
    "                batch_size, timestep.shape[1], num_ada_params, -1\n",
    "            )\n",
    "            if self.adaptive_norm == \"single_scale_shift\":\n",
    "                shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n",
    "                    ada_values.unbind(dim=2)\n",
    "                )\n",
    "                norm_hidden_states = norm_hidden_states * (1 + scale_msa) + shift_msa\n",
    "            else:\n",
    "                scale_msa, gate_msa, scale_mlp, gate_mlp = ada_values.unbind(dim=2)\n",
    "                norm_hidden_states = norm_hidden_states * (1 + scale_msa)\n",
    "\n",
    "                \n",
    "        elif self.adaptive_norm == \"none\":\n",
    "            scale_msa, gate_msa, scale_mlp, gate_mlp = None, None, None, None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown adaptive norm type: {self.adaptive_norm}\")\n",
    "\n",
    "        norm_hidden_states = norm_hidden_states.squeeze(\n",
    "            1\n",
    "        )  # TODO: Check if this is needed\n",
    "\n",
    "        # 1. Prepare GLIGEN inputs\n",
    "        cross_attention_kwargs = (\n",
    "            cross_attention_kwargs.copy() if cross_attention_kwargs is not None else {}\n",
    "        )\n",
    "\n",
    "        attn_output = self.attn1(\n",
    "            norm_hidden_states,\n",
    "            freqs_cis=freqs_cis,\n",
    "            encoder_hidden_states=(\n",
    "                encoder_hidden_states if self.only_cross_attention else None\n",
    "            ),\n",
    "            attention_mask=attention_mask,\n",
    "            **cross_attention_kwargs,\n",
    "        )\n",
    "        if gate_msa is not None:\n",
    "            attn_output = gate_msa * attn_output\n",
    "\n",
    "        hidden_states = attn_output + hidden_states\n",
    "        if hidden_states.ndim == 4:\n",
    "            hidden_states = hidden_states.squeeze(1)\n",
    "\n",
    "        # 3. Cross-Attention\n",
    "        if self.attn2 is not None:\n",
    "            if self.adaptive_norm == \"none\":\n",
    "                attn_input = self.attn2_norm(hidden_states)\n",
    "            else:\n",
    "                attn_input = hidden_states\n",
    "            attn_output = self.attn2(\n",
    "                attn_input,\n",
    "                freqs_cis=freqs_cis,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                **cross_attention_kwargs,\n",
    "            )\n",
    "            hidden_states = attn_output + hidden_states\n",
    "\n",
    "        # 4. Feed-forward\n",
    "        norm_hidden_states = self.norm2(hidden_states)\n",
    "        if self.adaptive_norm == \"single_scale_shift\":\n",
    "            norm_hidden_states = norm_hidden_states * (1 + scale_mlp) + shift_mlp\n",
    "        elif self.adaptive_norm == \"single_scale\":\n",
    "            norm_hidden_states = norm_hidden_states * (1 + scale_mlp)\n",
    "        elif self.adaptive_norm == \"none\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown adaptive norm type: {self.adaptive_norm}\")\n",
    "\n",
    "        if self._chunk_size is not None:\n",
    "            # \"feed_forward_chunk_size\" can be used to save memory\n",
    "            ff_output = _chunked_feed_forward(\n",
    "                self.ff, norm_hidden_states, self._chunk_dim, self._chunk_size\n",
    "            )\n",
    "        else:\n",
    "            ff_output = self.ff(norm_hidden_states)\n",
    "        if gate_mlp is not None:\n",
    "            ff_output = gate_mlp * ff_output\n",
    "\n",
    "        hidden_states = ff_output + hidden_states\n",
    "        if hidden_states.ndim == 4:\n",
    "            hidden_states = hidden_states.squeeze(1)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "(tensor([[1, 2, 3]]), tensor([[4, 5, 6]]), tensor([[7, 8, 9]]))\n",
      "(tensor([[1, 4, 7]]), tensor([[2, 5, 8]]), tensor([[3, 6, 9]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[1, 2, 3],\n",
    "                           [4, 5, 6],\n",
    "                           [7, 8, 9]]])\n",
    "print(a)\n",
    "\n",
    "\n",
    "b = a.unbind(dim=1)\n",
    "c = a.unbind(dim=2)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 4])\n",
      "torch.Size([10, 4, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.randn(10, 3, 4)\n",
    "print(input.size())\n",
    "mat2 = torch.randn(10, 4, 5)\n",
    "print(mat2.size())\n",
    "res = torch.bmm(input, mat2)\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from importlib import import_module\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers.models.activations import GEGLU, GELU, ApproximateGELU\n",
    "from diffusers.models.attention import _chunked_feed_forward\n",
    "from diffusers.models.attention_processor import (\n",
    "    LoRAAttnAddedKVProcessor,\n",
    "    LoRAAttnProcessor,\n",
    "    LoRAAttnProcessor2_0,\n",
    "    LoRAXFormersAttnProcessor,\n",
    "    SpatialNorm,\n",
    ")\n",
    "from diffusers.models.lora import LoRACompatibleLinear\n",
    "from diffusers.models.normalization import RMSNorm\n",
    "from diffusers.utils import deprecate, logging\n",
    "from diffusers.utils.torch_utils import maybe_allow_in_graph\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "try:\n",
    "    from torch_xla.experimental.custom_kernel import flash_attention\n",
    "except ImportError:\n",
    "    # workaround for automatic tests. Currently this function is manually patched\n",
    "    # to the torch_xla lib on setup of container\n",
    "    pass\n",
    "\n",
    "# code adapted from  https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "@maybe_allow_in_graph\n",
    "class BasicTransformerBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    A basic Transformer block.\n",
    "\n",
    "    Parameters:\n",
    "        dim (`int`): The number of channels in the input and output.\n",
    "        num_attention_heads (`int`): The number of heads to use for multi-head attention.\n",
    "        attention_head_dim (`int`): The number of channels in each head.\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
    "        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.\n",
    "        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n",
    "        num_embeds_ada_norm (:\n",
    "            obj: `int`, *optional*): The number of diffusion steps used during training. See `Transformer2DModel`.\n",
    "        attention_bias (:\n",
    "            obj: `bool`, *optional*, defaults to `False`): Configure if the attentions should contain a bias parameter.\n",
    "        only_cross_attention (`bool`, *optional*):\n",
    "            Whether to use only cross-attention layers. In this case two cross attention layers are used.\n",
    "        double_self_attention (`bool`, *optional*):\n",
    "            Whether to use two self-attention layers. In this case no cross attention layers are used.\n",
    "        upcast_attention (`bool`, *optional*):\n",
    "            Whether to upcast the attention computation to float32. This is useful for mixed precision training.\n",
    "        norm_elementwise_affine (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to use learnable elementwise affine parameters for normalization.\n",
    "        qk_norm (`str`, *optional*, defaults to None):\n",
    "            Set to 'layer_norm' or `rms_norm` to perform query and key normalization.\n",
    "        adaptive_norm (`str`, *optional*, defaults to `\"single_scale_shift\"`):\n",
    "            The type of adaptive norm to use. Can be `\"single_scale_shift\"`, `\"single_scale\"` or \"none\".\n",
    "        standardization_norm (`str`, *optional*, defaults to `\"layer_norm\"`):\n",
    "            The type of pre-normalization to use. Can be `\"layer_norm\"` or `\"rms_norm\"`.\n",
    "        final_dropout (`bool` *optional*, defaults to False):\n",
    "            Whether to apply a final dropout after the last feed-forward layer.\n",
    "        attention_type (`str`, *optional*, defaults to `\"default\"`):\n",
    "            The type of attention to use. Can be `\"default\"` or `\"gated\"` or `\"gated-text-image\"`.\n",
    "        positional_embeddings (`str`, *optional*, defaults to `None`):\n",
    "            The type of positional embeddings to apply to.\n",
    "        num_positional_embeddings (`int`, *optional*, defaults to `None`):\n",
    "            The maximum number of positional embeddings to apply.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_attention_heads: int,\n",
    "        attention_head_dim: int,\n",
    "        dropout=0.0,\n",
    "        cross_attention_dim: Optional[int] = None,\n",
    "        activation_fn: str = \"geglu\",\n",
    "        num_embeds_ada_norm: Optional[int] = None,  # pylint: disable=unused-argument\n",
    "        attention_bias: bool = False,\n",
    "        only_cross_attention: bool = False,\n",
    "        double_self_attention: bool = False,\n",
    "        upcast_attention: bool = False,\n",
    "        norm_elementwise_affine: bool = True,\n",
    "        adaptive_norm: str = \"single_scale_shift\",  # 'single_scale_shift', 'single_scale' or 'none'\n",
    "        standardization_norm: str = \"layer_norm\",  # 'layer_norm' or 'rms_norm'\n",
    "        norm_eps: float = 1e-5,\n",
    "        qk_norm: Optional[str] = None,\n",
    "        final_dropout: bool = False,\n",
    "        attention_type: str = \"default\",  # pylint: disable=unused-argument\n",
    "        ff_inner_dim: Optional[int] = None,\n",
    "        ff_bias: bool = True,\n",
    "        attention_out_bias: bool = True,\n",
    "        use_tpu_flash_attention: bool = False,\n",
    "        use_rope: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.only_cross_attention = only_cross_attention\n",
    "        self.use_tpu_flash_attention = use_tpu_flash_attention\n",
    "        self.adaptive_norm = adaptive_norm\n",
    "\n",
    "        assert standardization_norm in [\"layer_norm\", \"rms_norm\"]\n",
    "        assert adaptive_norm in [\"single_scale_shift\", \"single_scale\", \"none\"]\n",
    "\n",
    "        make_norm_layer = (\n",
    "            nn.LayerNorm if standardization_norm == \"layer_norm\" else RMSNorm\n",
    "        )\n",
    "\n",
    "        # Define 3 blocks. Each block has its own normalization layer.\n",
    "        # 1. Self-Attn\n",
    "        self.norm1 = make_norm_layer(\n",
    "            dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps\n",
    "        )\n",
    "\n",
    "        self.attn1 = Attention(\n",
    "            query_dim=dim,\n",
    "            heads=num_attention_heads,\n",
    "            dim_head=attention_head_dim,\n",
    "            dropout=dropout,\n",
    "            bias=attention_bias,\n",
    "            cross_attention_dim=cross_attention_dim if only_cross_attention else None,\n",
    "            upcast_attention=upcast_attention,\n",
    "            out_bias=attention_out_bias,\n",
    "            use_tpu_flash_attention=use_tpu_flash_attention,\n",
    "            qk_norm=qk_norm,\n",
    "            use_rope=use_rope,\n",
    "        )\n",
    "\n",
    "        # 2. Cross-Attn\n",
    "        if cross_attention_dim is not None or double_self_attention:\n",
    "            self.attn2 = Attention(\n",
    "                query_dim=dim,\n",
    "                cross_attention_dim=(\n",
    "                    cross_attention_dim if not double_self_attention else None\n",
    "                ),\n",
    "                heads=num_attention_heads,\n",
    "                dim_head=attention_head_dim,\n",
    "                dropout=dropout,\n",
    "                bias=attention_bias,\n",
    "                upcast_attention=upcast_attention,\n",
    "                out_bias=attention_out_bias,\n",
    "                use_tpu_flash_attention=use_tpu_flash_attention,\n",
    "                qk_norm=qk_norm,\n",
    "                use_rope=use_rope,\n",
    "            )  # is self-attn if encoder_hidden_states is none\n",
    "\n",
    "            if adaptive_norm == \"none\":\n",
    "                self.attn2_norm = make_norm_layer(\n",
    "                    dim, norm_eps, norm_elementwise_affine\n",
    "                )\n",
    "        else:\n",
    "            self.attn2 = None\n",
    "            self.attn2_norm = None\n",
    "\n",
    "        self.norm2 = make_norm_layer(dim, norm_eps, norm_elementwise_affine)\n",
    "\n",
    "        # 3. Feed-forward\n",
    "        self.ff = FeedForward(\n",
    "            dim,\n",
    "            dropout=dropout,\n",
    "            activation_fn=activation_fn,\n",
    "            final_dropout=final_dropout,\n",
    "            inner_dim=ff_inner_dim,\n",
    "            bias=ff_bias,\n",
    "        )\n",
    "\n",
    "        # 5. Scale-shift for PixArt-Alpha.\n",
    "        if adaptive_norm != \"none\":\n",
    "            num_ada_params = 4 if adaptive_norm == \"single_scale\" else 6\n",
    "            self.scale_shift_table = nn.Parameter(\n",
    "                torch.randn(num_ada_params, dim) / dim**0.5\n",
    "            )\n",
    "\n",
    "        # let chunk size default to None\n",
    "        self._chunk_size = None\n",
    "        self._chunk_dim = 0\n",
    "\n",
    "    def set_use_tpu_flash_attention(self):\n",
    "        r\"\"\"\n",
    "        Function sets the flag in this object and propagates down the children. The flag will enforce the usage of TPU\n",
    "        attention kernel.\n",
    "        \"\"\"\n",
    "        self.use_tpu_flash_attention = True\n",
    "        self.attn1.set_use_tpu_flash_attention()\n",
    "        self.attn2.set_use_tpu_flash_attention()\n",
    "\n",
    "    def set_chunk_feed_forward(self, chunk_size: Optional[int], dim: int = 0):\n",
    "        # Sets chunk feed-forward\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_dim = dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        freqs_cis: Optional[Tuple[torch.FloatTensor, torch.FloatTensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        cross_attention_kwargs: Dict[str, Any] = None,\n",
    "        class_labels: Optional[torch.LongTensor] = None,\n",
    "        added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        if cross_attention_kwargs is not None:\n",
    "            if cross_attention_kwargs.get(\"scale\", None) is not None:\n",
    "                logger.warning(\n",
    "                    \"Passing `scale` to `cross_attention_kwargs` is depcrecated. `scale` will be ignored.\"\n",
    "                )\n",
    "\n",
    "        # Notice that normalization is always applied before the real computation in the following blocks.\n",
    "        # 0. Self-Attention\n",
    "        batch_size = hidden_states.shape[0]\n",
    "\n",
    "        norm_hidden_states = self.norm1(hidden_states)\n",
    "\n",
    "        # Apply ada_norm_single\n",
    "        if self.adaptive_norm in [\"single_scale_shift\", \"single_scale\"]:\n",
    "            assert timestep.ndim == 3  # [batch, 1 or num_tokens, embedding_dim]\n",
    "            num_ada_params = self.scale_shift_table.shape[0]\n",
    "            ada_values = self.scale_shift_table[None, None] + timestep.reshape(\n",
    "                batch_size, timestep.shape[1], num_ada_params, -1\n",
    "            )\n",
    "            if self.adaptive_norm == \"single_scale_shift\":\n",
    "                shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n",
    "                    ada_values.unbind(dim=2)\n",
    "                )\n",
    "                norm_hidden_states = norm_hidden_states * (1 + scale_msa) + shift_msa\n",
    "            else:\n",
    "                scale_msa, gate_msa, scale_mlp, gate_mlp = ada_values.unbind(dim=2)\n",
    "                norm_hidden_states = norm_hidden_states * (1 + scale_msa)\n",
    "        elif self.adaptive_norm == \"none\":\n",
    "            scale_msa, gate_msa, scale_mlp, gate_mlp = None, None, None, None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown adaptive norm type: {self.adaptive_norm}\")\n",
    "\n",
    "        norm_hidden_states = norm_hidden_states.squeeze(\n",
    "            1\n",
    "        )  # TODO: Check if this is needed\n",
    "\n",
    "        # 1. Prepare GLIGEN inputs\n",
    "        cross_attention_kwargs = (\n",
    "            cross_attention_kwargs.copy() if cross_attention_kwargs is not None else {}\n",
    "        )\n",
    "\n",
    "        attn_output = self.attn1(\n",
    "            norm_hidden_states,\n",
    "            freqs_cis=freqs_cis,\n",
    "            encoder_hidden_states=(\n",
    "                encoder_hidden_states if self.only_cross_attention else None\n",
    "            ),\n",
    "            attention_mask=attention_mask,\n",
    "            **cross_attention_kwargs,\n",
    "        )\n",
    "        if gate_msa is not None:\n",
    "            attn_output = gate_msa * attn_output\n",
    "\n",
    "        hidden_states = attn_output + hidden_states\n",
    "        if hidden_states.ndim == 4:\n",
    "            hidden_states = hidden_states.squeeze(1)\n",
    "\n",
    "        # 3. Cross-Attention\n",
    "        if self.attn2 is not None:\n",
    "            if self.adaptive_norm == \"none\":\n",
    "                attn_input = self.attn2_norm(hidden_states)\n",
    "            else:\n",
    "                attn_input = hidden_states\n",
    "            attn_output = self.attn2(\n",
    "                attn_input,\n",
    "                freqs_cis=freqs_cis,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                **cross_attention_kwargs,\n",
    "            )\n",
    "            hidden_states = attn_output + hidden_states\n",
    "\n",
    "        # 4. Feed-forward\n",
    "        norm_hidden_states = self.norm2(hidden_states)\n",
    "        if self.adaptive_norm == \"single_scale_shift\":\n",
    "            norm_hidden_states = norm_hidden_states * (1 + scale_mlp) + shift_mlp\n",
    "        elif self.adaptive_norm == \"single_scale\":\n",
    "            norm_hidden_states = norm_hidden_states * (1 + scale_mlp)\n",
    "        elif self.adaptive_norm == \"none\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown adaptive norm type: {self.adaptive_norm}\")\n",
    "\n",
    "        if self._chunk_size is not None:\n",
    "            # \"feed_forward_chunk_size\" can be used to save memory\n",
    "            ff_output = _chunked_feed_forward(\n",
    "                self.ff, norm_hidden_states, self._chunk_dim, self._chunk_size\n",
    "            )\n",
    "        else:\n",
    "            ff_output = self.ff(norm_hidden_states)\n",
    "        if gate_mlp is not None:\n",
    "            ff_output = gate_mlp * ff_output\n",
    "\n",
    "        hidden_states = ff_output + hidden_states\n",
    "        if hidden_states.ndim == 4:\n",
    "            hidden_states = hidden_states.squeeze(1)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "@maybe_allow_in_graph\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    A cross attention layer.\n",
    "\n",
    "    Parameters:\n",
    "        query_dim (`int`):\n",
    "            The number of channels in the query.\n",
    "        cross_attention_dim (`int`, *optional*):\n",
    "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
    "        heads (`int`,  *optional*, defaults to 8):\n",
    "            The number of heads to use for multi-head attention.\n",
    "        dim_head (`int`,  *optional*, defaults to 64):\n",
    "            The number of channels in each head.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probability to use.\n",
    "        bias (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
    "        upcast_attention (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` to upcast the attention computation to `float32`.\n",
    "        upcast_softmax (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` to upcast the softmax computation to `float32`.\n",
    "        cross_attention_norm (`str`, *optional*, defaults to `None`):\n",
    "            The type of normalization to use for the cross attention. Can be `None`, `layer_norm`, or `group_norm`.\n",
    "        cross_attention_norm_num_groups (`int`, *optional*, defaults to 32):\n",
    "            The number of groups to use for the group norm in the cross attention.\n",
    "        added_kv_proj_dim (`int`, *optional*, defaults to `None`):\n",
    "            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n",
    "        norm_num_groups (`int`, *optional*, defaults to `None`):\n",
    "            The number of groups to use for the group norm in the attention.\n",
    "        spatial_norm_dim (`int`, *optional*, defaults to `None`):\n",
    "            The number of channels to use for the spatial normalization.\n",
    "        out_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Set to `True` to use a bias in the output linear layer.\n",
    "        scale_qk (`bool`, *optional*, defaults to `True`):\n",
    "            Set to `True` to scale the query and key by `1 / sqrt(dim_head)`.\n",
    "        qk_norm (`str`, *optional*, defaults to None):\n",
    "            Set to 'layer_norm' or `rms_norm` to perform query and key normalization.\n",
    "        only_cross_attention (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` to only use cross attention and not added_kv_proj_dim. Can only be set to `True` if\n",
    "            `added_kv_proj_dim` is not `None`.\n",
    "        eps (`float`, *optional*, defaults to 1e-5):\n",
    "            An additional value added to the denominator in group normalization that is used for numerical stability.\n",
    "        rescale_output_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor to rescale the output by dividing it with this value.\n",
    "        residual_connection (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` to add the residual connection to the output.\n",
    "        _from_deprecated_attn_block (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` if the attention block is loaded from a deprecated state dict.\n",
    "        processor (`AttnProcessor`, *optional*, defaults to `None`):\n",
    "            The attention processor to use. If `None`, defaults to `AttnProcessor2_0` if `torch 2.x` is used and\n",
    "            `AttnProcessor` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_dim: int,\n",
    "        cross_attention_dim: Optional[int] = None,\n",
    "        heads: int = 8,\n",
    "        dim_head: int = 64,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        upcast_attention: bool = False,\n",
    "        upcast_softmax: bool = False,\n",
    "        cross_attention_norm: Optional[str] = None,\n",
    "        cross_attention_norm_num_groups: int = 32,\n",
    "        added_kv_proj_dim: Optional[int] = None,\n",
    "        norm_num_groups: Optional[int] = None,\n",
    "        spatial_norm_dim: Optional[int] = None,\n",
    "        out_bias: bool = True,\n",
    "        scale_qk: bool = True,\n",
    "        qk_norm: Optional[str] = None,\n",
    "        only_cross_attention: bool = False,\n",
    "        eps: float = 1e-5,\n",
    "        rescale_output_factor: float = 1.0,\n",
    "        residual_connection: bool = False,\n",
    "        _from_deprecated_attn_block: bool = False,\n",
    "        processor: Optional[\"AttnProcessor\"] = None,\n",
    "        out_dim: int = None,\n",
    "        use_tpu_flash_attention: bool = False,\n",
    "        use_rope: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.inner_dim = out_dim if out_dim is not None else dim_head * heads\n",
    "        self.query_dim = query_dim\n",
    "        self.use_bias = bias\n",
    "        self.is_cross_attention = cross_attention_dim is not None\n",
    "        self.cross_attention_dim = (\n",
    "            cross_attention_dim if cross_attention_dim is not None else query_dim\n",
    "        )\n",
    "        self.upcast_attention = upcast_attention\n",
    "        self.upcast_softmax = upcast_softmax\n",
    "        self.rescale_output_factor = rescale_output_factor\n",
    "        self.residual_connection = residual_connection\n",
    "        self.dropout = dropout\n",
    "        self.fused_projections = False\n",
    "        self.out_dim = out_dim if out_dim is not None else query_dim\n",
    "        self.use_tpu_flash_attention = use_tpu_flash_attention\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        # we make use of this private variable to know whether this class is loaded\n",
    "        # with an deprecated state dict so that we can convert it on the fly\n",
    "        self._from_deprecated_attn_block = _from_deprecated_attn_block\n",
    "\n",
    "        self.scale_qk = scale_qk\n",
    "        self.scale = dim_head**-0.5 if self.scale_qk else 1.0\n",
    "\n",
    "        if qk_norm is None:\n",
    "            self.q_norm = nn.Identity()\n",
    "            self.k_norm = nn.Identity()\n",
    "        elif qk_norm == \"rms_norm\":\n",
    "            self.q_norm = RMSNorm(dim_head * heads, eps=1e-5)\n",
    "            self.k_norm = RMSNorm(dim_head * heads, eps=1e-5)\n",
    "        elif qk_norm == \"layer_norm\":\n",
    "            self.q_norm = nn.LayerNorm(dim_head * heads, eps=1e-5)\n",
    "            self.k_norm = nn.LayerNorm(dim_head * heads, eps=1e-5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported qk_norm method: {qk_norm}\")\n",
    "\n",
    "        self.heads = out_dim // dim_head if out_dim is not None else heads\n",
    "        # for slice_size > 0 the attention score computation\n",
    "        # is split across the batch axis to save memory\n",
    "        # You can set slice_size with `set_attention_slice`\n",
    "        self.sliceable_head_dim = heads\n",
    "\n",
    "        self.added_kv_proj_dim = added_kv_proj_dim\n",
    "        self.only_cross_attention = only_cross_attention\n",
    "\n",
    "        if self.added_kv_proj_dim is None and self.only_cross_attention:\n",
    "            raise ValueError(\n",
    "                \"`only_cross_attention` can only be set to True if `added_kv_proj_dim` is not None. Make sure to set either `only_cross_attention=False` or define `added_kv_proj_dim`.\"\n",
    "            )\n",
    "\n",
    "        if norm_num_groups is not None:\n",
    "            self.group_norm = nn.GroupNorm(\n",
    "                num_channels=query_dim, num_groups=norm_num_groups, eps=eps, affine=True\n",
    "            )\n",
    "        else:\n",
    "            self.group_norm = None\n",
    "\n",
    "        if spatial_norm_dim is not None:\n",
    "            self.spatial_norm = SpatialNorm(\n",
    "                f_channels=query_dim, zq_channels=spatial_norm_dim\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_norm = None\n",
    "\n",
    "        if cross_attention_norm is None:\n",
    "            self.norm_cross = None\n",
    "        elif cross_attention_norm == \"layer_norm\":\n",
    "            self.norm_cross = nn.LayerNorm(self.cross_attention_dim)\n",
    "        elif cross_attention_norm == \"group_norm\":\n",
    "            if self.added_kv_proj_dim is not None:\n",
    "                # The given `encoder_hidden_states` are initially of shape\n",
    "                # (batch_size, seq_len, added_kv_proj_dim) before being projected\n",
    "                # to (batch_size, seq_len, cross_attention_dim). The norm is applied\n",
    "                # before the projection, so we need to use `added_kv_proj_dim` as\n",
    "                # the number of channels for the group norm.\n",
    "                norm_cross_num_channels = added_kv_proj_dim\n",
    "            else:\n",
    "                norm_cross_num_channels = self.cross_attention_dim\n",
    "\n",
    "            self.norm_cross = nn.GroupNorm(\n",
    "                num_channels=norm_cross_num_channels,\n",
    "                num_groups=cross_attention_norm_num_groups,\n",
    "                eps=1e-5,\n",
    "                affine=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"unknown cross_attention_norm: {cross_attention_norm}. Should be None, 'layer_norm' or 'group_norm'\"\n",
    "            )\n",
    "\n",
    "        linear_cls = nn.Linear\n",
    "\n",
    "        self.linear_cls = linear_cls\n",
    "        self.to_q = linear_cls(query_dim, self.inner_dim, bias=bias)\n",
    "\n",
    "        if not self.only_cross_attention:\n",
    "            # only relevant for the `AddedKVProcessor` classes\n",
    "            self.to_k = linear_cls(self.cross_attention_dim, self.inner_dim, bias=bias)\n",
    "            self.to_v = linear_cls(self.cross_attention_dim, self.inner_dim, bias=bias)\n",
    "        else:\n",
    "            self.to_k = None\n",
    "            self.to_v = None\n",
    "\n",
    "        if self.added_kv_proj_dim is not None:\n",
    "            self.add_k_proj = linear_cls(added_kv_proj_dim, self.inner_dim)\n",
    "            self.add_v_proj = linear_cls(added_kv_proj_dim, self.inner_dim)\n",
    "\n",
    "        self.to_out = nn.ModuleList([])\n",
    "        self.to_out.append(linear_cls(self.inner_dim, self.out_dim, bias=out_bias))\n",
    "        self.to_out.append(nn.Dropout(dropout))\n",
    "\n",
    "        # set attention processor\n",
    "        # We use the AttnProcessor2_0 by default when torch 2.x is used which uses\n",
    "        # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention\n",
    "        # but only if it has the default `scale` argument. TODO remove scale_qk check when we move to torch 2.1\n",
    "        if processor is None:\n",
    "            processor = AttnProcessor2_0()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_use_tpu_flash_attention(self):\n",
    "        r\"\"\"\n",
    "        Function sets the flag in this object. The flag will enforce the usage of TPU attention kernel.\n",
    "        \"\"\"\n",
    "        self.use_tpu_flash_attention = True\n",
    "\n",
    "    def set_processor(self, processor: \"AttnProcessor\") -> None:\n",
    "        r\"\"\"\n",
    "        Set the attention processor to use.\n",
    "\n",
    "        Args:\n",
    "            processor (`AttnProcessor`):\n",
    "                The attention processor to use.\n",
    "        \"\"\"\n",
    "        # if current processor is in `self._modules` and if passed `processor` is not, we need to\n",
    "        # pop `processor` from `self._modules`\n",
    "        if (\n",
    "            hasattr(self, \"processor\")\n",
    "            and isinstance(self.processor, torch.nn.Module)\n",
    "            and not isinstance(processor, torch.nn.Module)\n",
    "        ):\n",
    "            logger.info(\n",
    "                f\"You are removing possibly trained weights of {self.processor} with {processor}\"\n",
    "            )\n",
    "            self._modules.pop(\"processor\")\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(\n",
    "        self, return_deprecated_lora: bool = False\n",
    "    ) -> \"AttentionProcessor\":  # noqa: F821\n",
    "        r\"\"\"\n",
    "        Get the attention processor in use.\n",
    "\n",
    "        Args:\n",
    "            return_deprecated_lora (`bool`, *optional*, defaults to `False`):\n",
    "                Set to `True` to return the deprecated LoRA attention processor.\n",
    "\n",
    "        Returns:\n",
    "            \"AttentionProcessor\": The attention processor in use.\n",
    "        \"\"\"\n",
    "        if not return_deprecated_lora:\n",
    "            return self.processor\n",
    "\n",
    "        # TODO(Sayak, Patrick). The rest of the function is needed to ensure backwards compatible\n",
    "        # serialization format for LoRA Attention Processors. It should be deleted once the integration\n",
    "        # with PEFT is completed.\n",
    "        is_lora_activated = {\n",
    "            name: module.lora_layer is not None\n",
    "            for name, module in self.named_modules()\n",
    "            if hasattr(module, \"lora_layer\")\n",
    "        }\n",
    "\n",
    "        # 1. if no layer has a LoRA activated we can return the processor as usual\n",
    "        if not any(is_lora_activated.values()):\n",
    "            return self.processor\n",
    "\n",
    "        # If doesn't apply LoRA do `add_k_proj` or `add_v_proj`\n",
    "        is_lora_activated.pop(\"add_k_proj\", None)\n",
    "        is_lora_activated.pop(\"add_v_proj\", None)\n",
    "        # 2. else it is not posssible that only some layers have LoRA activated\n",
    "        if not all(is_lora_activated.values()):\n",
    "            raise ValueError(\n",
    "                f\"Make sure that either all layers or no layers have LoRA activated, but have {is_lora_activated}\"\n",
    "            )\n",
    "\n",
    "        # 3. And we need to merge the current LoRA layers into the corresponding LoRA attention processor\n",
    "        non_lora_processor_cls_name = self.processor.__class__.__name__\n",
    "        lora_processor_cls = getattr(\n",
    "            import_module(__name__), \"LoRA\" + non_lora_processor_cls_name\n",
    "        )\n",
    "\n",
    "        hidden_size = self.inner_dim\n",
    "\n",
    "        # now create a LoRA attention processor from the LoRA layers\n",
    "        if lora_processor_cls in [\n",
    "            LoRAAttnProcessor,\n",
    "            LoRAAttnProcessor2_0,\n",
    "            LoRAXFormersAttnProcessor,\n",
    "        ]:\n",
    "            kwargs = {\n",
    "                \"cross_attention_dim\": self.cross_attention_dim,\n",
    "                \"rank\": self.to_q.lora_layer.rank,\n",
    "                \"network_alpha\": self.to_q.lora_layer.network_alpha,\n",
    "                \"q_rank\": self.to_q.lora_layer.rank,\n",
    "                \"q_hidden_size\": self.to_q.lora_layer.out_features,\n",
    "                \"k_rank\": self.to_k.lora_layer.rank,\n",
    "                \"k_hidden_size\": self.to_k.lora_layer.out_features,\n",
    "                \"v_rank\": self.to_v.lora_layer.rank,\n",
    "                \"v_hidden_size\": self.to_v.lora_layer.out_features,\n",
    "                \"out_rank\": self.to_out[0].lora_layer.rank,\n",
    "                \"out_hidden_size\": self.to_out[0].lora_layer.out_features,\n",
    "            }\n",
    "\n",
    "            if hasattr(self.processor, \"attention_op\"):\n",
    "                kwargs[\"attention_op\"] = self.processor.attention_op\n",
    "\n",
    "            lora_processor = lora_processor_cls(hidden_size, **kwargs)\n",
    "            lora_processor.to_q_lora.load_state_dict(self.to_q.lora_layer.state_dict())\n",
    "            lora_processor.to_k_lora.load_state_dict(self.to_k.lora_layer.state_dict())\n",
    "            lora_processor.to_v_lora.load_state_dict(self.to_v.lora_layer.state_dict())\n",
    "            lora_processor.to_out_lora.load_state_dict(\n",
    "                self.to_out[0].lora_layer.state_dict()\n",
    "            )\n",
    "        elif lora_processor_cls == LoRAAttnAddedKVProcessor:\n",
    "            lora_processor = lora_processor_cls(\n",
    "                hidden_size,\n",
    "                cross_attention_dim=self.add_k_proj.weight.shape[0],\n",
    "                rank=self.to_q.lora_layer.rank,\n",
    "                network_alpha=self.to_q.lora_layer.network_alpha,\n",
    "            )\n",
    "            lora_processor.to_q_lora.load_state_dict(self.to_q.lora_layer.state_dict())\n",
    "            lora_processor.to_k_lora.load_state_dict(self.to_k.lora_layer.state_dict())\n",
    "            lora_processor.to_v_lora.load_state_dict(self.to_v.lora_layer.state_dict())\n",
    "            lora_processor.to_out_lora.load_state_dict(\n",
    "                self.to_out[0].lora_layer.state_dict()\n",
    "            )\n",
    "\n",
    "            # only save if used\n",
    "            if self.add_k_proj.lora_layer is not None:\n",
    "                lora_processor.add_k_proj_lora.load_state_dict(\n",
    "                    self.add_k_proj.lora_layer.state_dict()\n",
    "                )\n",
    "                lora_processor.add_v_proj_lora.load_state_dict(\n",
    "                    self.add_v_proj.lora_layer.state_dict()\n",
    "                )\n",
    "            else:\n",
    "                lora_processor.add_k_proj_lora = None\n",
    "                lora_processor.add_v_proj_lora = None\n",
    "        else:\n",
    "            raise ValueError(f\"{lora_processor_cls} does not exist.\")\n",
    "\n",
    "        return lora_processor\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        freqs_cis: Optional[Tuple[torch.FloatTensor, torch.FloatTensor]] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        **cross_attention_kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        The forward method of the `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (`torch.Tensor`):\n",
    "                The hidden states of the query.\n",
    "            encoder_hidden_states (`torch.Tensor`, *optional*):\n",
    "                The hidden states of the encoder.\n",
    "            attention_mask (`torch.Tensor`, *optional*):\n",
    "                The attention mask to use. If `None`, no mask is applied.\n",
    "            **cross_attention_kwargs:\n",
    "                Additional keyword arguments to pass along to the cross attention.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The output of the attention layer.\n",
    "        \"\"\"\n",
    "        # The `Attention` class can call different attention processors / attention functions\n",
    "        # here we simply pass along all tensors to the selected processor class\n",
    "        # For standard processors that are defined here, `**cross_attention_kwargs` is empty\n",
    "\n",
    "        attn_parameters = set(\n",
    "            inspect.signature(self.processor.__call__).parameters.keys()\n",
    "        )\n",
    "        unused_kwargs = [\n",
    "            k for k, _ in cross_attention_kwargs.items() if k not in attn_parameters\n",
    "        ]\n",
    "        if len(unused_kwargs) > 0:\n",
    "            logger.warning(\n",
    "                f\"cross_attention_kwargs {unused_kwargs} are not expected by\"\n",
    "                f\" {self.processor.__class__.__name__} and will be ignored.\"\n",
    "            )\n",
    "        cross_attention_kwargs = {\n",
    "            k: w for k, w in cross_attention_kwargs.items() if k in attn_parameters\n",
    "        }\n",
    "\n",
    "        return self.processor(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            freqs_cis=freqs_cis,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            **cross_attention_kwargs,\n",
    "        )\n",
    "\n",
    "    def batch_to_head_dim(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Reshape the tensor from `[batch_size, seq_len, dim]` to `[batch_size // heads, seq_len, dim * heads]`. `heads`\n",
    "        is the number of heads initialized while constructing the `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            tensor (`torch.Tensor`): The tensor to reshape.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        head_size = self.heads\n",
    "        batch_size, seq_len, dim = tensor.shape\n",
    "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
    "        tensor = tensor.permute(0, 2, 1, 3).reshape(\n",
    "            batch_size // head_size, seq_len, dim * head_size\n",
    "        )\n",
    "        return tensor\n",
    "\n",
    "    def head_to_batch_dim(self, tensor: torch.Tensor, out_dim: int = 3) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Reshape the tensor from `[batch_size, seq_len, dim]` to `[batch_size, seq_len, heads, dim // heads]` `heads` is\n",
    "        the number of heads initialized while constructing the `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            tensor (`torch.Tensor`): The tensor to reshape.\n",
    "            out_dim (`int`, *optional*, defaults to `3`): The output dimension of the tensor. If `3`, the tensor is\n",
    "                reshaped to `[batch_size * heads, seq_len, dim // heads]`.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The reshaped tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        head_size = self.heads\n",
    "        if tensor.ndim == 3:\n",
    "            batch_size, seq_len, dim = tensor.shape\n",
    "            extra_dim = 1\n",
    "        else:\n",
    "            batch_size, extra_dim, seq_len, dim = tensor.shape\n",
    "        tensor = tensor.reshape(\n",
    "            batch_size, seq_len * extra_dim, head_size, dim // head_size\n",
    "        )\n",
    "        tensor = tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "        if out_dim == 3:\n",
    "            tensor = tensor.reshape(\n",
    "                batch_size * head_size, seq_len * extra_dim, dim // head_size\n",
    "            )\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def get_attention_scores(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Compute the attention scores.\n",
    "\n",
    "        Args:\n",
    "            query (`torch.Tensor`): The query tensor.\n",
    "            key (`torch.Tensor`): The key tensor.\n",
    "            attention_mask (`torch.Tensor`, *optional*): The attention mask to use. If `None`, no mask is applied.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The attention probabilities/scores.\n",
    "        \"\"\"\n",
    "        dtype = query.dtype\n",
    "        if self.upcast_attention:\n",
    "            query = query.float()\n",
    "            key = key.float()\n",
    "\n",
    "        if attention_mask is None:\n",
    "            baddbmm_input = torch.empty(\n",
    "                query.shape[0],\n",
    "                query.shape[1],\n",
    "                key.shape[1],\n",
    "                dtype=query.dtype,\n",
    "                device=query.device,\n",
    "            )\n",
    "            beta = 0\n",
    "        else:\n",
    "            baddbmm_input = attention_mask\n",
    "            beta = 1\n",
    "\n",
    "        attention_scores = torch.baddbmm(\n",
    "            baddbmm_input,\n",
    "            query,\n",
    "            key.transpose(-1, -2),\n",
    "            beta=beta,\n",
    "            alpha=self.scale,\n",
    "        )\n",
    "        del baddbmm_input\n",
    "\n",
    "        if self.upcast_softmax:\n",
    "            attention_scores = attention_scores.float()\n",
    "\n",
    "        attention_probs = attention_scores.softmax(dim=-1)\n",
    "        del attention_scores\n",
    "\n",
    "        attention_probs = attention_probs.to(dtype)\n",
    "\n",
    "        return attention_probs\n",
    "\n",
    "    def prepare_attention_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        target_length: int,\n",
    "        batch_size: int,\n",
    "        out_dim: int = 3,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Prepare the attention mask for the attention computation.\n",
    "\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor`):\n",
    "                The attention mask to prepare.\n",
    "            target_length (`int`):\n",
    "                The target length of the attention mask. This is the length of the attention mask after padding.\n",
    "            batch_size (`int`):\n",
    "                The batch size, which is used to repeat the attention mask.\n",
    "            out_dim (`int`, *optional*, defaults to `3`):\n",
    "                The output dimension of the attention mask. Can be either `3` or `4`.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The prepared attention mask.\n",
    "        \"\"\"\n",
    "        head_size = self.heads\n",
    "        if attention_mask is None:\n",
    "            return attention_mask\n",
    "\n",
    "        current_length: int = attention_mask.shape[-1]\n",
    "        if current_length != target_length:\n",
    "            if attention_mask.device.type == \"mps\":\n",
    "                # HACK: MPS: Does not support padding by greater than dimension of input tensor.\n",
    "                # Instead, we can manually construct the padding tensor.\n",
    "                padding_shape = (\n",
    "                    attention_mask.shape[0],\n",
    "                    attention_mask.shape[1],\n",
    "                    target_length,\n",
    "                )\n",
    "                padding = torch.zeros(\n",
    "                    padding_shape,\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device,\n",
    "                )\n",
    "                attention_mask = torch.cat([attention_mask, padding], dim=2)\n",
    "            else:\n",
    "                # TODO: for pipelines such as stable-diffusion, padding cross-attn mask:\n",
    "                #       we want to instead pad by (0, remaining_length), where remaining_length is:\n",
    "                #       remaining_length: int = target_length - current_length\n",
    "                # TODO: re-enable tests/models/test_models_unet_2d_condition.py#test_model_xattn_padding\n",
    "                attention_mask = F.pad(attention_mask, (0, target_length), value=0.0)\n",
    "\n",
    "        if out_dim == 3:\n",
    "            if attention_mask.shape[0] < batch_size * head_size:\n",
    "                attention_mask = attention_mask.repeat_interleave(head_size, dim=0)\n",
    "        elif out_dim == 4:\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "            attention_mask = attention_mask.repeat_interleave(head_size, dim=1)\n",
    "\n",
    "        return attention_mask\n",
    "\n",
    "    def norm_encoder_hidden_states(\n",
    "        self, encoder_hidden_states: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Normalize the encoder hidden states. Requires `self.norm_cross` to be specified when constructing the\n",
    "        `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            encoder_hidden_states (`torch.Tensor`): Hidden states of the encoder.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The normalized encoder hidden states.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.norm_cross is not None\n",
    "        ), \"self.norm_cross must be defined to call self.norm_encoder_hidden_states\"\n",
    "\n",
    "        if isinstance(self.norm_cross, nn.LayerNorm):\n",
    "            encoder_hidden_states = self.norm_cross(encoder_hidden_states)\n",
    "        elif isinstance(self.norm_cross, nn.GroupNorm):\n",
    "            # Group norm norms along the channels dimension and expects\n",
    "            # input to be in the shape of (N, C, *). In this case, we want\n",
    "            # to norm along the hidden dimension, so we need to move\n",
    "            # (batch_size, sequence_length, hidden_size) ->\n",
    "            # (batch_size, hidden_size, sequence_length)\n",
    "            encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n",
    "            encoder_hidden_states = self.norm_cross(encoder_hidden_states)\n",
    "            encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        return encoder_hidden_states\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_rotary_emb(\n",
    "        input_tensor: torch.Tensor,\n",
    "        freqs_cis: Tuple[torch.FloatTensor, torch.FloatTensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        cos_freqs = freqs_cis[0]\n",
    "        sin_freqs = freqs_cis[1]\n",
    "\n",
    "        t_dup = rearrange(input_tensor, \"... (d r) -> ... d r\", r=2)\n",
    "        t1, t2 = t_dup.unbind(dim=-1)\n",
    "        t_dup = torch.stack((-t2, t1), dim=-1)\n",
    "        input_tensor_rot = rearrange(t_dup, \"... d r -> ... (d r)\")\n",
    "\n",
    "        out = input_tensor * cos_freqs + input_tensor_rot * sin_freqs\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttnProcessor2_0:\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        freqs_cis: Tuple[torch.FloatTensor, torch.FloatTensor],\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        temb: Optional[torch.FloatTensor] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.FloatTensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        residual = hidden_states\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(\n",
    "                batch_size, channel, height * width\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape\n",
    "            if encoder_hidden_states is None\n",
    "            else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if (attention_mask is not None) and (not attn.use_tpu_flash_attention):\n",
    "            attention_mask = attn.prepare_attention_mask(\n",
    "                attention_mask, sequence_length, batch_size\n",
    "            )\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(\n",
    "                batch_size, attn.heads, -1, attention_mask.shape[-1]\n",
    "            )\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(\n",
    "                1, 2\n",
    "            )\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "        query = attn.q_norm(query)\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            if attn.norm_cross:\n",
    "                encoder_hidden_states = attn.norm_encoder_hidden_states(\n",
    "                    encoder_hidden_states\n",
    "                )\n",
    "            key = attn.to_k(encoder_hidden_states)\n",
    "            key = attn.k_norm(key)\n",
    "        else:  # if no context provided do self-attention\n",
    "            encoder_hidden_states = hidden_states\n",
    "            key = attn.to_k(hidden_states)\n",
    "            key = attn.k_norm(key)\n",
    "            if attn.use_rope:\n",
    "                key = attn.apply_rotary_emb(key, freqs_cis)\n",
    "                query = attn.apply_rotary_emb(query, freqs_cis)\n",
    "\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "        if attn.use_tpu_flash_attention:  # use tpu attention offload 'flash attention'\n",
    "            q_segment_indexes = None\n",
    "            if (\n",
    "                attention_mask is not None\n",
    "            ):  # if mask is required need to tune both segmenIds fields\n",
    "                # attention_mask = torch.squeeze(attention_mask).to(torch.float32)\n",
    "                attention_mask = attention_mask.to(torch.float32)\n",
    "                q_segment_indexes = torch.ones(\n",
    "                    batch_size, query.shape[2], device=query.device, dtype=torch.float32\n",
    "                )\n",
    "                assert (\n",
    "                    attention_mask.shape[1] == key.shape[2]\n",
    "                ), f\"ERROR: KEY SHAPE must be same as attention mask [{key.shape[2]}, {attention_mask.shape[1]}]\"\n",
    "\n",
    "            assert (\n",
    "                query.shape[2] % 128 == 0\n",
    "            ), f\"ERROR: QUERY SHAPE must be divisible by 128 (TPU limitation) [{query.shape[2]}]\"\n",
    "            assert (\n",
    "                key.shape[2] % 128 == 0\n",
    "            ), f\"ERROR: KEY SHAPE must be divisible by 128 (TPU limitation) [{key.shape[2]}]\"\n",
    "\n",
    "            # run the TPU kernel implemented in jax with pallas\n",
    "            hidden_states = flash_attention(\n",
    "                q=query,\n",
    "                k=key,\n",
    "                v=value,\n",
    "                q_segment_ids=q_segment_indexes,\n",
    "                kv_segment_ids=attention_mask,\n",
    "                sm_scale=attn.scale,\n",
    "            )\n",
    "        else:\n",
    "            hidden_states = F.scaled_dot_product_attention(\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                attn_mask=attention_mask,\n",
    "                dropout_p=0.0,\n",
    "                is_causal=False,\n",
    "            )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(\n",
    "            batch_size, -1, attn.heads * head_dim\n",
    "        )\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(\n",
    "                batch_size, channel, height, width\n",
    "            )\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class AttnProcessor:\n",
    "    r\"\"\"\n",
    "    Default processor for performing attention-related computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        temb: Optional[torch.FloatTensor] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(\n",
    "                batch_size, channel, height * width\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape\n",
    "            if encoder_hidden_states is None\n",
    "            else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(\n",
    "            attention_mask, sequence_length, batch_size\n",
    "        )\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(\n",
    "                1, 2\n",
    "            )\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(\n",
    "                encoder_hidden_states\n",
    "            )\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        query = attn.q_norm(query)\n",
    "        key = attn.k_norm(key)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(\n",
    "                batch_size, channel, height, width\n",
    "            )\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    r\"\"\"\n",
    "    A feed-forward layer.\n",
    "\n",
    "    Parameters:\n",
    "        dim (`int`): The number of channels in the input.\n",
    "        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.\n",
    "        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
    "        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n",
    "        final_dropout (`bool` *optional*, defaults to False): Apply a final dropout.\n",
    "        bias (`bool`, defaults to True): Whether to use a bias in the linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dim_out: Optional[int] = None,\n",
    "        mult: int = 4,\n",
    "        dropout: float = 0.0,\n",
    "        activation_fn: str = \"geglu\",\n",
    "        final_dropout: bool = False,\n",
    "        inner_dim=None,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if inner_dim is None:\n",
    "            inner_dim = int(dim * mult)\n",
    "        dim_out = dim_out if dim_out is not None else dim\n",
    "        linear_cls = nn.Linear\n",
    "\n",
    "        if activation_fn == \"gelu\":\n",
    "            act_fn = GELU(dim, inner_dim, bias=bias)\n",
    "        elif activation_fn == \"gelu-approximate\":\n",
    "            act_fn = GELU(dim, inner_dim, approximate=\"tanh\", bias=bias)\n",
    "        elif activation_fn == \"geglu\":\n",
    "            act_fn = GEGLU(dim, inner_dim, bias=bias)\n",
    "        elif activation_fn == \"geglu-approximate\":\n",
    "            act_fn = ApproximateGELU(dim, inner_dim, bias=bias)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "\n",
    "        self.net = nn.ModuleList([])\n",
    "        # project in\n",
    "        self.net.append(act_fn)\n",
    "        # project dropout\n",
    "        self.net.append(nn.Dropout(dropout))\n",
    "        # project out\n",
    "        self.net.append(linear_cls(inner_dim, dim_out, bias=bias))\n",
    "        # FF as used in Vision Transformer, MLP-Mixer, etc. have a final dropout\n",
    "        if final_dropout:\n",
    "            self.net.append(nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n",
    "        compatible_cls = (GEGLU, LoRACompatibleLinear)\n",
    "        for module in self.net:\n",
    "            if isinstance(module, compatible_cls):\n",
    "                hidden_states = module(hidden_states, scale)\n",
    "            else:\n",
    "                hidden_states = module(hidden_states)\n",
    "        return hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyramid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
