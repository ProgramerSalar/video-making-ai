{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed485de78406453e8111ecd5872d933c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manjusha Kumari\\.cache\\huggingface\\hub\\models--Lightricks--LTX-Video. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module diffusers has no attribute LTXPipeline",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionPipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightricks/LTX-Video\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAstronaut in a jungle, cold color palette, muted colors, detailed, 8k\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m pipe(prompt)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:725\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    722\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe provided pretrained_model_name_or_path \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    723\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    724\u001b[0m         )\n\u001b[1;32m--> 725\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m     cached_folder \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1375\u001b[0m, in \u001b[0;36mDiffusionPipeline.download\u001b[1;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contains custom code in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.py, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(k,\u001b[38;5;250m \u001b[39mv)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,v\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcustom_components\u001b[38;5;241m.\u001b[39mitems()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which must be executed to correctly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload the model. You can inspect the repository content at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,v\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcustom_components\u001b[38;5;241m.\u001b[39mitems()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m     )\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# retrieve passed components that should not be downloaded\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m pipeline_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_pipeline_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_pipe_from_hub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhub_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_class_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1386\u001b[0m expected_components, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_signature_keys(pipeline_class)\n\u001b[0;32m   1387\u001b[0m passed_components \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m expected_components \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs]\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:370\u001b[0m, in \u001b[0;36m_get_pipeline_class\u001b[1;34m(class_obj, config, load_connected_pipeline, custom_pipeline, repo_id, hub_revision, class_name, cache_dir, revision)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class name could not be found in the configuration file. Please make sure to pass the correct `class_name`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[0;32m    368\u001b[0m class_name \u001b[38;5;241m=\u001b[39m class_name[\u001b[38;5;241m4\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m class_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlax\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m class_name\n\u001b[1;32m--> 370\u001b[0m pipeline_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdiffusers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_connected_pipeline:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_connected_pipeline\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\.conda\\envs\\pyramid\\lib\\site-packages\\diffusers\\utils\\import_utils.py:846\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    844\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mAttributeError\u001b[0m: module diffusers has no attribute LTXPipeline"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"Lightricks/LTX-Video\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipe(prompt).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_linear_nd(x):\n",
    "    \"\"\"\n",
    "    Reshape the tensor to (batch_size, -1) suitable for a fully connected layer.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (batch_size, *).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped tensor of shape (batch_size, -1).\n",
    "    \"\"\"\n",
    "    return x.view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DualConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n",
    "        super(DualConv3d, self).__init__()\n",
    "        kernel_size = kernel_size\n",
    "        \n",
    "        if kernel_size == (1, 1, 1):\n",
    "            raise ValueError(\"Kernel size should be greater than 1. Use make_linear_nd instead.\")\n",
    "        \n",
    "        weight1 = nn.Parameter(torch.randn(out_channels, in_channels, *kernel_size))\n",
    "        bias1 = nn.Parameter(torch.randn(out_channels)) if bias else None\n",
    "        stride1 = stride\n",
    "        padding1 = padding\n",
    "        dilation1 = (1, 1, 1)\n",
    "        groups = 1\n",
    "\n",
    "    def forward(self, x, use_conv3d=False):\n",
    "        if kernel_size == (1, 1, 1):\n",
    "            x = make_linear_nd(x)\n",
    "            # Here, you would normally pass this reshaped tensor to a linear layer\n",
    "            return x\n",
    "        else:\n",
    "            x = F.conv3d(x, weight1, bias1, stride1, padding1, dilation1, groups)\n",
    "            return x\n",
    "\n",
    "# Example usage\n",
    "in_channels = 3\n",
    "out_channels = 5\n",
    "kernel_size = (3, 3, 3)\n",
    "stride = (2, 2, 2)\n",
    "padding = (1, 1, 1)\n",
    "\n",
    "# Create an instance of DualConv3d\n",
    "dual_conv3d = DualConv3d(in_channels, out_channels, kernel_size, stride, padding, bias=True)\n",
    "\n",
    "# Example input tensor\n",
    "test_input = torch.randn(1, 3, 10, 10, 10)\n",
    "\n",
    "# Perform forward pass\n",
    "output = dual_conv3d(test_input, use_conv3d=True)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import torch \n",
    "\n",
    "# a = torch.sqrt(2)\n",
    "# print(a)\n",
    "\n",
    "b = math.sqrt(2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor a:\n",
      "tensor([[[[[2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.]]]]])\n",
      "Original Tensor b:\n",
      "tensor([[[[[5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.]]]]])\n",
      "blend_extent 2\n",
      "y 0\n",
      "object: ->  tensor([[[[5., 5., 5., 5.]]]])\n",
      "calulate -2\n",
      "blend extend 2\n",
      "blend_extent2:  tensor([[[[[2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.]]]]])\n",
      "blend_extent3:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent4:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent5:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent6:  tensor([[[[7., 7., 7., 7.]]]])\n",
      "multiple tensor([[[[0., 0., 0., 0.]]]])\n",
      "blend_extent7:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "hello: tensor([[[[5., 5., 5., 5.]]]]) = tensor([[[[2., 2., 2., 2.]]]])\n",
      "y 1\n",
      "object: ->  tensor([[[[5., 5., 5., 5.]]]])\n",
      "calulate -1\n",
      "blend extend 2\n",
      "blend_extent2:  tensor([[[[[2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.],\n",
      "           [2., 2., 2., 2.]]]]])\n",
      "blend_extent3:  tensor([[[[2., 2., 2., 2.]]]])\n",
      "blend_extent4:  tensor([[[[0., 0., 0., 0.]]]])\n",
      "blend_extent5:  tensor([[[[1., 1., 1., 1.]]]])\n",
      "blend_extent6:  tensor([[[[6., 6., 6., 6.]]]])\n",
      "multiple tensor([[[[5., 5., 5., 5.]]]])\n",
      "blend_extent7:  tensor([[[[6., 6., 6., 6.]]]])\n",
      "hello: tensor([[[[5., 5., 5., 5.]]]]) = tensor([[[[2., 2., 2., 2.]]]])\n",
      "Blended Tensor:\n",
      "tensor([[[[[5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.],\n",
      "           [5., 5., 5., 5.]]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def blend_v(\n",
    "        a: torch.Tensor,\n",
    "        b: torch.Tensor,\n",
    "        blend_extent: int\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    blend_extent = min(a.shape[3], b.shape[3], blend_extent)\n",
    "    print(\"blend_extent\", blend_extent)\n",
    "\n",
    "    for y in range(blend_extent):\n",
    "        print(\"y\", y)\n",
    "        print(\"object: -> \", b[:, :, :, y, :])\n",
    "        print(\"calulate\", -blend_extent + y)\n",
    "        # print(\"blend extent2 \", a.size()) -> torch.Size([1, 1, 1, 4, 4])\n",
    "        print(\"blend extend\", blend_extent)\n",
    "        print(\"blend_extent2: \", a[:, :, :, :, :])\n",
    "        print(\"blend_extent3: \", a[:, :, :, -blend_extent + y, :])   # tensor([[[[2., 2., 2., 2.]]]])\n",
    "        print(\"blend_extent4: \", a[:, :, :, -blend_extent + y, :] * (1 - y))  # y = 0 -> 1 - 0 = 1 But y = 1 -> 1 - 1 = 0\n",
    "        print(\"blend_extent5: \", a[:, :, :, -blend_extent + y] * (1 - y / blend_extent))  # 1 / 2 = 1 and 0 / 2 = 0 \n",
    "        # tensor([[[[2., 2., 2., 2.]]]]) + tensor([[[[5., 5., 5., 5.]]]])  =  tensor([[[[7., 7., 7., 7.]]]])\n",
    "        print(\"blend_extent6: \", a[:, :, :, -blend_extent + y] * (1 - y / blend_extent) + b[:, :, :, y, :]) \n",
    "        print(\"multiple\", b[:, :, :, y, :] * y) # 5 * 0 = 0 and 5 * 1 = 5 \n",
    "        print(\"blend_extent7: \", a[:, :, :, -blend_extent + y] * (1 - y / blend_extent) + b[:, :, :, y, :] * y)  # 5 * 0 = 0 and 5 * 1 = 5 \n",
    "        \n",
    "\n",
    "        \n",
    "        print(f\"hello: {b[:, :, :, y, :]} = {a[:, :, :, -blend_extent + y, :]}\")\n",
    "\n",
    "    return b\n",
    "\n",
    "# Create sample tensors\n",
    "a = torch.ones((1, 1, 1, 4, 4)) * 2  # Tensor a filled with value 2\n",
    "b = torch.ones((1, 1, 1, 4, 4)) * 5  # Tensor b filled with value 5\n",
    "\n",
    "# Display original tensors\n",
    "print(\"Original Tensor a:\")\n",
    "print(a)\n",
    "\n",
    "print(\"Original Tensor b:\")\n",
    "print(b)\n",
    "\n",
    "# Set blend_extent\n",
    "blend_extent = 2\n",
    "\n",
    "# Blend tensors vertically\n",
    "blended_tensor = blend_v(a, b, blend_extent)\n",
    "\n",
    "# Display blended tensor\n",
    "print(\"Blended Tensor:\")\n",
    "print(blended_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_v(\n",
    "            self,\n",
    "            a: torch.Tensor,\n",
    "            b: torch.Tensor,\n",
    "            blend_extent: int\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        blend_extent = min(a.shape[3],\n",
    "                           b.shape[3],\n",
    "                           blend_extent\n",
    "                           )\n",
    "        \n",
    "\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (\n",
    "                1 - y / blend_extent\n",
    "            ) + b[:, :, :, y, :] * (y / blend_extent)\n",
    "\n",
    "\n",
    "        return b \n",
    "    \n",
    "\n",
    "def blend_h(\n",
    "            self,\n",
    "            a: torch.Tensor,\n",
    "            b: torch.Tensor,\n",
    "            blend_extent:  int\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        blend_extent = min(a.shape[4], b.shape[4], blend_extent)\n",
    "\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (\n",
    "                1 - x / blend_extent\n",
    "            ) + b[:, :, :, :, x] * (x / blend_extent)\n",
    "    \n",
    "\n",
    "        return b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "overlap_size:  3\n",
      "blend_extent:  1\n",
      "row_limit:  3\n",
      "i:  0\n",
      "j:  0\n",
      "before tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "before add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "          [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "          [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "          [-0.3214, -0.6163,  0.8686, -1.5710]],\n",
      "\n",
      "         [[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "          [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "          [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "          [-1.4594, -1.3954, -0.1143,  0.5692]],\n",
      "\n",
      "         [[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "          [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "          [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "          [-1.5739, -0.4865,  1.5061,  0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[-0.1515, -0.4670,  0.2821,  0.7793],\n",
      "          [ 0.5981, -0.0590,  0.2559, -0.4094],\n",
      "          [-0.0795, -0.4326, -0.2921,  0.1694],\n",
      "          [-0.2159,  0.0672, -0.1232, -0.8193]],\n",
      "\n",
      "         [[ 0.0028,  0.3754,  0.0967, -0.2321],\n",
      "          [ 0.0911, -0.0075, -0.2960,  0.5578],\n",
      "          [-0.2440, -0.1506,  0.1978,  0.4953],\n",
      "          [-0.5388,  0.2812,  0.3634,  0.9035]],\n",
      "\n",
      "         [[ 0.4003,  0.2176, -0.2258, -0.0688],\n",
      "          [-0.1504,  0.2110, -0.3487,  0.6612],\n",
      "          [-1.2308, -0.1979,  0.7831, -1.0373],\n",
      "          [-0.2329, -0.7870, -0.0583, -0.3236]],\n",
      "\n",
      "         [[-0.5736, -1.1146, -0.0926,  0.3791],\n",
      "          [-1.3218, -0.1257, -0.2714, -0.1148],\n",
      "          [-0.3103, -0.1509, -0.0782,  0.7312],\n",
      "          [ 0.3630, -0.2393,  0.4769,  0.7472]],\n",
      "\n",
      "         [[-0.0914,  0.5633,  0.2613,  0.2330],\n",
      "          [ 0.1870,  0.5642, -0.7003, -0.6296],\n",
      "          [-0.1041, -0.6836, -0.3631,  0.4847],\n",
      "          [-0.0754, -0.1411,  0.1807,  0.5265]],\n",
      "\n",
      "         [[-0.5094, -0.5004, -0.0652,  0.0996],\n",
      "          [ 0.2430,  0.1559,  0.3274, -0.0515],\n",
      "          [ 0.6533, -0.0671,  0.5773,  0.7474],\n",
      "          [ 0.9911,  0.5063,  0.3010, -0.0045]],\n",
      "\n",
      "         [[ 0.8989,  0.0880,  0.2075,  0.6340],\n",
      "          [ 0.3365,  0.3409,  0.3967,  0.3274],\n",
      "          [-0.0387,  1.2388,  0.4723, -0.9630],\n",
      "          [ 0.6008,  0.2854,  0.2148,  0.0209]],\n",
      "\n",
      "         [[ 0.5665,  0.2106, -0.1043, -0.1559],\n",
      "          [ 0.3953, -0.1291,  0.7229,  0.4070],\n",
      "          [ 0.1641, -0.5198,  0.8538, -0.9724],\n",
      "          [-0.0484, -0.4151,  0.1318,  0.4480]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)]\n",
      "j:  3\n",
      "before tile:  tensor([[[[[ 0.1764],\n",
      "           [-1.2856],\n",
      "           [-1.5204],\n",
      "           [-1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-0.0374],\n",
      "           [ 0.9624],\n",
      "           [-1.4286],\n",
      "           [ 0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0122],\n",
      "           [ 1.2837],\n",
      "           [-0.8858],\n",
      "           [ 0.1685]]]]])\n",
      "before add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[ 1.5201,  0.5186,  0.7231,  0.1764],\n",
      "           [ 0.9649,  0.3002,  0.2164, -1.2856],\n",
      "           [-0.0429, -0.9251, -1.8161, -1.5204],\n",
      "           [-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9377, -1.5524, -0.4212, -0.0374],\n",
      "           [-1.1621, -0.7998,  1.3137,  0.9624],\n",
      "           [-1.6743,  0.2641, -0.1526, -1.4286],\n",
      "           [-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3877,  0.3580,  0.1190,  0.0122],\n",
      "           [ 1.0036,  0.4932, -0.4532,  1.2837],\n",
      "           [ 0.2997,  1.6076, -1.2261, -0.8858],\n",
      "           [-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[ 0.1764],\n",
      "          [-1.2856],\n",
      "          [-1.5204],\n",
      "          [-1.5710]],\n",
      "\n",
      "         [[-0.0374],\n",
      "          [ 0.9624],\n",
      "          [-1.4286],\n",
      "          [ 0.5692]],\n",
      "\n",
      "         [[ 0.0122],\n",
      "          [ 1.2837],\n",
      "          [-0.8858],\n",
      "          [ 0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[ 0.4931],\n",
      "          [-0.5834],\n",
      "          [ 0.3503],\n",
      "          [-0.5842]],\n",
      "\n",
      "         [[ 0.0355],\n",
      "          [ 0.5459],\n",
      "          [ 0.5298],\n",
      "          [ 0.5515]],\n",
      "\n",
      "         [[ 0.1415],\n",
      "          [-0.1029],\n",
      "          [-0.3908],\n",
      "          [-0.1082]],\n",
      "\n",
      "         [[ 0.2223],\n",
      "          [-0.1802],\n",
      "          [ 0.2950],\n",
      "          [ 0.8458]],\n",
      "\n",
      "         [[ 0.0744],\n",
      "          [ 0.0293],\n",
      "          [ 0.1612],\n",
      "          [-0.0822]],\n",
      "\n",
      "         [[ 0.1301],\n",
      "          [-0.0904],\n",
      "          [ 0.3509],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[ 0.3890],\n",
      "          [-0.0360],\n",
      "          [-0.0852],\n",
      "          [ 0.0974]],\n",
      "\n",
      "         [[-0.1284],\n",
      "          [ 0.1792],\n",
      "          [-0.3948],\n",
      "          [ 0.0181]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "rows:  [[tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)]]\n",
      "i:  3\n",
      "j:  0\n",
      "before tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "before add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[-0.3214, -0.6163,  0.8686, -1.5710]],\n",
      "\n",
      "         [[-1.4594, -1.3954, -0.1143,  0.5692]],\n",
      "\n",
      "         [[-1.5739, -0.4865,  1.5061,  0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[ 0.1617, -0.0016,  0.1255, -0.3699]],\n",
      "\n",
      "         [[-0.2799, -0.2459, -0.4161,  0.5723]],\n",
      "\n",
      "         [[-0.2936, -0.6766, -0.6531, -0.1541]],\n",
      "\n",
      "         [[ 0.3507,  0.4176,  0.2967, -0.0978]],\n",
      "\n",
      "         [[-0.3842,  0.0382,  0.1290, -0.1349]],\n",
      "\n",
      "         [[ 0.5060,  0.7361,  0.1276, -0.0778]],\n",
      "\n",
      "         [[ 0.1075, -0.2011,  0.2138,  0.1081]],\n",
      "\n",
      "         [[-0.1419, -0.6838, -0.2915,  0.4977]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)]\n",
      "j:  3\n",
      "before tile:  tensor([[[[[-1.5710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5692]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1685]]]]])\n",
      "before add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "after add tile:  tensor([[[[[-0.3214, -0.6163,  0.8686, -1.5710]]],\n",
      "\n",
      "\n",
      "         [[[-1.4594, -1.3954, -0.1143,  0.5692]]],\n",
      "\n",
      "\n",
      "         [[[-1.5739, -0.4865,  1.5061,  0.1685]]]]])\n",
      "After tile squeeze:  tensor([[[[-1.5710]],\n",
      "\n",
      "         [[ 0.5692]],\n",
      "\n",
      "         [[ 0.1685]]]])\n",
      "tile pass in the encoder:  tensor([[[[-0.3839]],\n",
      "\n",
      "         [[ 0.4825]],\n",
      "\n",
      "         [[-0.2518]],\n",
      "\n",
      "         [[ 0.2341]],\n",
      "\n",
      "         [[-0.2903]],\n",
      "\n",
      "         [[ 0.1898]],\n",
      "\n",
      "         [[-0.1427]],\n",
      "\n",
      "         [[ 0.1970]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tile pass in the quant_conv:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "row:  [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "rows:  [[tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)], [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)]]\n",
      "i:  0\n",
      "row:  [tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "j:  0\n",
      "tile:  tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0949],\n",
      "          [ 0.3998,  0.2536, -0.1874, -0.0124],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1703],\n",
      "          [-0.1840,  0.2904,  0.0170,  0.2668]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.0609],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.1505],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.4613],\n",
      "          [-0.4368, -0.5991, -0.1339, -0.0392]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.2565],\n",
      "          [ 0.0147,  0.3724,  0.2178, -0.1790],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.2842],\n",
      "          [ 0.8210,  0.3090,  0.3638,  0.6075]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.1528],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.1107],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1411],\n",
      "          [ 0.3728, -0.0888, -0.1358, -0.6147]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.0452],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.3493],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.0886],\n",
      "          [ 0.0144, -0.0232, -0.2147, -0.3564]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.3792],\n",
      "          [-0.2310,  0.0728,  0.1766,  0.2594],\n",
      "          [-0.3059,  0.1741,  0.5677, -0.3493],\n",
      "          [ 0.4401, -0.1477,  0.2596,  0.0824]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.0463],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1437],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.1420],\n",
      "          [-0.3420, -0.0988, -0.1617, -0.0652]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.1574],\n",
      "          [ 0.0786, -0.1515,  0.2320,  0.3645],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.8765],\n",
      "          [-0.1525, -0.2977, -0.1310, -0.1999]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[ 0.2532,  0.7386,  0.2219],\n",
      "          [ 0.3998,  0.2536, -0.1874],\n",
      "          [ 0.1930,  0.2137, -0.2245]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834],\n",
      "          [-0.7181, -0.1948, -0.6362],\n",
      "          [-0.9344, -0.2942, -0.2364]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724],\n",
      "          [ 0.0147,  0.3724,  0.2178],\n",
      "          [ 0.6500,  0.3631,  0.1851]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471],\n",
      "          [-0.2169, -0.0104, -0.0123],\n",
      "          [-0.1756,  0.2040,  0.0609]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841],\n",
      "          [ 0.5858,  0.1225,  0.1780],\n",
      "          [ 0.3924, -0.3587, -0.1027]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647],\n",
      "          [-0.2310,  0.0728,  0.1766],\n",
      "          [-0.3059,  0.1741,  0.5677]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191],\n",
      "          [-0.5909, -0.2410, -0.4142],\n",
      "          [-0.4341,  0.3380, -0.5479]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749],\n",
      "          [ 0.0786, -0.1515,  0.2320],\n",
      "          [-0.2956,  0.0692,  0.4095]]]], grad_fn=<SliceBackward0>)]\n",
      "j:  1\n",
      "tile:  tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)\n",
      "j is greater then 0:  tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252],\n",
      "          [-0.0269]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615],\n",
      "          [-0.0700]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813],\n",
      "          [ 0.3910]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322],\n",
      "          [-0.1647]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754],\n",
      "          [-0.4941]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036],\n",
      "          [ 0.2688]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874],\n",
      "          [ 0.0058]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565],\n",
      "          [-0.1487]]]], grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[ 0.2532,  0.7386,  0.2219],\n",
      "          [ 0.3998,  0.2536, -0.1874],\n",
      "          [ 0.1930,  0.2137, -0.2245]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834],\n",
      "          [-0.7181, -0.1948, -0.6362],\n",
      "          [-0.9344, -0.2942, -0.2364]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724],\n",
      "          [ 0.0147,  0.3724,  0.2178],\n",
      "          [ 0.6500,  0.3631,  0.1851]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471],\n",
      "          [-0.2169, -0.0104, -0.0123],\n",
      "          [-0.1756,  0.2040,  0.0609]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841],\n",
      "          [ 0.5858,  0.1225,  0.1780],\n",
      "          [ 0.3924, -0.3587, -0.1027]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647],\n",
      "          [-0.2310,  0.0728,  0.1766],\n",
      "          [-0.3059,  0.1741,  0.5677]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191],\n",
      "          [-0.5909, -0.2410, -0.4142],\n",
      "          [-0.4341,  0.3380, -0.5479]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749],\n",
      "          [ 0.0786, -0.1515,  0.2320],\n",
      "          [-0.2956,  0.0692,  0.4095]]]], grad_fn=<SliceBackward0>), tensor([[[[-0.0481],\n",
      "          [ 0.3454],\n",
      "          [ 0.1252]],\n",
      "\n",
      "         [[-0.1040],\n",
      "          [-0.3530],\n",
      "          [-0.2615]],\n",
      "\n",
      "         [[ 0.0861],\n",
      "          [ 0.1800],\n",
      "          [ 0.0813]],\n",
      "\n",
      "         [[ 0.0912],\n",
      "          [-0.3859],\n",
      "          [-0.1322]],\n",
      "\n",
      "         [[-0.1034],\n",
      "          [-0.0680],\n",
      "          [-0.1754]],\n",
      "\n",
      "         [[ 0.2868],\n",
      "          [-0.1997],\n",
      "          [ 0.0036]],\n",
      "\n",
      "         [[-0.1057],\n",
      "          [-0.1596],\n",
      "          [-0.0874]],\n",
      "\n",
      "         [[-0.0838],\n",
      "          [-0.0340],\n",
      "          [-0.3565]]]], grad_fn=<SliceBackward0>)]\n",
      "i:  1\n",
      "row:  [tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "j:  0\n",
      "tile:  tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "i is greater then 0:  tensor([[[[-0.2058,  0.0037,  0.0731,  0.2117]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943, -0.3385]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165,  0.1576]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425, -0.4233]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008, -0.1039]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410, -0.0481]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564, -0.1916]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290,  0.0790]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[-0.2058,  0.0037,  0.0731]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290]]]], grad_fn=<SliceBackward0>)]\n",
      "j:  1\n",
      "tile:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "i is greater then 0:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "j is greater then 0:  tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<ConvolutionBackward0>)\n",
      "result_row:  [tensor([[[[-0.2058,  0.0037,  0.0731]],\n",
      "\n",
      "         [[-0.4181, -0.5201, -0.3943]],\n",
      "\n",
      "         [[ 0.2741,  0.4804,  0.5165]],\n",
      "\n",
      "         [[ 0.2128,  0.2020,  0.0425]],\n",
      "\n",
      "         [[-0.0567,  0.0205,  0.1008]],\n",
      "\n",
      "         [[ 0.1920, -0.0642, -0.0410]],\n",
      "\n",
      "         [[-0.2007, -0.1787, -0.0564]],\n",
      "\n",
      "         [[-0.1439, -0.5123, -0.3290]]]], grad_fn=<SliceBackward0>), tensor([[[[ 0.0588]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         [[-0.2567]],\n",
      "\n",
      "         [[-0.2048]],\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[-0.1698]],\n",
      "\n",
      "         [[-0.0591]]]], grad_fn=<SliceBackward0>)]\n",
      "moment:  tensor([[[[ 0.2532,  0.7386,  0.2219, -0.0481],\n",
      "          [ 0.3998,  0.2536, -0.1874,  0.3454],\n",
      "          [ 0.1930,  0.2137, -0.2245,  0.1252],\n",
      "          [-0.2058,  0.0037,  0.0731,  0.0588]],\n",
      "\n",
      "         [[-0.1922, -0.4254, -0.2834, -0.1040],\n",
      "          [-0.7181, -0.1948, -0.6362, -0.3530],\n",
      "          [-0.9344, -0.2942, -0.2364, -0.2615],\n",
      "          [-0.4181, -0.5201, -0.3943, -0.3646]],\n",
      "\n",
      "         [[ 0.1484,  0.0841,  0.1724,  0.0861],\n",
      "          [ 0.0147,  0.3724,  0.2178,  0.1800],\n",
      "          [ 0.6500,  0.3631,  0.1851,  0.0813],\n",
      "          [ 0.2741,  0.4804,  0.5165,  0.1615]],\n",
      "\n",
      "         [[-0.1916, -0.4877, -0.1471,  0.0912],\n",
      "          [-0.2169, -0.0104, -0.0123, -0.3859],\n",
      "          [-0.1756,  0.2040,  0.0609, -0.1322],\n",
      "          [ 0.2128,  0.2020,  0.0425, -0.2567]],\n",
      "\n",
      "         [[ 0.0646,  0.3868,  0.0841, -0.1034],\n",
      "          [ 0.5858,  0.1225,  0.1780, -0.0680],\n",
      "          [ 0.3924, -0.3587, -0.1027, -0.1754],\n",
      "          [-0.0567,  0.0205,  0.1008, -0.2048]],\n",
      "\n",
      "         [[ 0.1285, -0.5101, -0.0647,  0.2868],\n",
      "          [-0.2310,  0.0728,  0.1766, -0.1997],\n",
      "          [-0.3059,  0.1741,  0.5677,  0.0036],\n",
      "          [ 0.1920, -0.0642, -0.0410, -0.0165]],\n",
      "\n",
      "         [[-0.1135, -0.2991, -0.1191, -0.1057],\n",
      "          [-0.5909, -0.2410, -0.4142, -0.1596],\n",
      "          [-0.4341,  0.3380, -0.5479, -0.0874],\n",
      "          [-0.2007, -0.1787, -0.0564, -0.1698]],\n",
      "\n",
      "         [[ 0.3623,  0.0433, -0.1749, -0.0838],\n",
      "          [ 0.0786, -0.1515,  0.2320, -0.0340],\n",
      "          [-0.2956,  0.0692,  0.4095, -0.3565],\n",
      "          [-0.1439, -0.5123, -0.3290, -0.0591]]]], grad_fn=<CatBackward0>)\n",
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, tile_sample_min_size, tile_overlap_factor, tile_latent_min_size):\n",
    "        self.tile_sample_min_size = tile_sample_min_size\n",
    "        self.tile_overlap_factor = tile_overlap_factor\n",
    "        self.tile_latent_min_size = tile_latent_min_size\n",
    "        self.encoder = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)  # Dummy encoder\n",
    "        self.quant_conv = nn.Conv2d(8, 8, kernel_size=1, stride=1, padding=0)  # Dummy quant conv\n",
    "\n",
    "    def blend_h(self, left_tile, current_tile, blend_extent):\n",
    "        # Dummy blend horizontal function\n",
    "        return current_tile\n",
    "\n",
    "    def blend_v(self, above_tile, current_tile, blend_extent):\n",
    "        # Dummy blend vertical function\n",
    "        return current_tile\n",
    "\n",
    "    def _hw_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True):\n",
    "        print(\"x\", x)\n",
    "        overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor))  \n",
    "        print(\"overlap_size: \", overlap_size) # 3\n",
    "\n",
    "        blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)\n",
    "        print(\"blend_extent: \", blend_extent)  # 1 \n",
    "\n",
    "\n",
    "        row_limit = self.tile_latent_min_size - blend_extent\n",
    "        print(\"row_limit: \", row_limit)  # torch.Size([1, 8, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[3], overlap_size):\n",
    "            print(\"i: \", i)\n",
    "\n",
    "            row = []\n",
    "            for j in range(0, x.shape[4], overlap_size):\n",
    "                print(\"j: \", j)\n",
    "                \n",
    "                tile = x[:, :, :, i: i + self.tile_sample_min_size, j: j + self.tile_sample_min_size]\n",
    "                print(\"before tile: \", tile)\n",
    "                print(\"before add tile: \", x[:, :, :, i:] )\n",
    "                print(\"after add tile: \", x[:, :, :, i: + self.tile_sample_min_size, ])\n",
    "\n",
    "                # Remove the extra dimension\n",
    "                tile = tile.squeeze(2)\n",
    "                print(\"After tile squeeze: \", tile)\n",
    "\n",
    "                tile = self.encoder(tile)\n",
    "                print(\"tile pass in the encoder: \", tile)\n",
    "\n",
    "                tile = self.quant_conv(tile)\n",
    "                print(\"tile pass in the quant_conv: \", tile)\n",
    "\n",
    "                row.append(tile)\n",
    "                print(\"row: \", row)\n",
    "\n",
    "            rows.append(row)\n",
    "            print(\"rows: \", rows)\n",
    "\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            print(\"i: \", i)\n",
    "            print(\"row: \", row)\n",
    "\n",
    "\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                print(\"j: \", j)\n",
    "                print(\"tile: \", tile)\n",
    "\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                    print(\"i is greater then 0: \", tile)\n",
    "\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                    print(\"j is greater then 0: \", tile)\n",
    "\n",
    "\n",
    "                result_row.append(tile[:, :, :row_limit, :row_limit])\n",
    "                print(\"result_row: \", result_row)\n",
    "\n",
    "            result_rows.append(torch.cat(result_row, dim=3))\n",
    "\n",
    "        moment = torch.cat(result_rows, dim=2)\n",
    "        print(\"moment: \", moment)\n",
    "        return moment\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tile_sample_min_size = 4\n",
    "tile_overlap_factor = 0.25\n",
    "tile_latent_min_size = 4\n",
    "\n",
    "encoder_decoder = EncoderDecoder(tile_sample_min_size, tile_overlap_factor, tile_latent_min_size)\n",
    "\n",
    "# Creating a dummy tensor with shape [1, 3, 1, 4, 4] (batch size, channels, depth, height, width)\n",
    "dummy_tensor = torch.randn(1, 3, 1, 4, 4)\n",
    "\n",
    "# Encode the dummy tensor\n",
    "encoded_tensor = encoder_decoder._hw_tiled_encode(dummy_tensor)\n",
    "\n",
    "print(encoded_tensor.shape)  # Check the shape of the encoded tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyramid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
